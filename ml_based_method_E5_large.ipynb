{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "785959980235465ead70d5f8d8bc5cfe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_44aa3b73963f47a0aacaedb96e9638d5",
              "IPY_MODEL_d667dfef3c8c49d1adc938678437841d",
              "IPY_MODEL_5bc671b6536b4b85abafe8f603c6aca6"
            ],
            "layout": "IPY_MODEL_a778f61cde804245b4cb768200594ab3"
          }
        },
        "44aa3b73963f47a0aacaedb96e9638d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f34bd8ec4e6f4ddc9bdedfb3234812a3",
            "placeholder": "​",
            "style": "IPY_MODEL_3af8aaa2a5dc4a8490074629830202cd",
            "value": "modules.json: 100%"
          }
        },
        "d667dfef3c8c49d1adc938678437841d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c3fc6ec4a3e44d1484d66390b01a4d46",
            "max": 387,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b9edf9b85b94489ca9f26948b46f7ad1",
            "value": 387
          }
        },
        "5bc671b6536b4b85abafe8f603c6aca6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9c67d320ee0e405787730154e295cdbb",
            "placeholder": "​",
            "style": "IPY_MODEL_33fe5d200c4f498abbebd5f5d72957e1",
            "value": " 387/387 [00:00&lt;00:00, 24.0kB/s]"
          }
        },
        "a778f61cde804245b4cb768200594ab3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f34bd8ec4e6f4ddc9bdedfb3234812a3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3af8aaa2a5dc4a8490074629830202cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c3fc6ec4a3e44d1484d66390b01a4d46": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b9edf9b85b94489ca9f26948b46f7ad1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9c67d320ee0e405787730154e295cdbb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "33fe5d200c4f498abbebd5f5d72957e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "17eecf9e012a46e5ae0744c01d72d9b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f0abb7ab38ad490da22875912957fcf8",
              "IPY_MODEL_06fc57a36b614d3880295be8f4537bad",
              "IPY_MODEL_6723794779fa4aa18ab1255937f16c32"
            ],
            "layout": "IPY_MODEL_2a94114405d44f6ba0f77086bdbfc0f3"
          }
        },
        "f0abb7ab38ad490da22875912957fcf8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_da6a5ca824b04a1b9857e0c6def3c7cd",
            "placeholder": "​",
            "style": "IPY_MODEL_bd803f6527d5418496852c5a6beb8ba8",
            "value": "README.md: 100%"
          }
        },
        "06fc57a36b614d3880295be8f4537bad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e657029ec8fe43db8aee61d6aff71a6b",
            "max": 160024,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_87e9c605ef9c47ef8e293ba2b18199b8",
            "value": 160024
          }
        },
        "6723794779fa4aa18ab1255937f16c32": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d2eacc640f764a08abecc15060286ac9",
            "placeholder": "​",
            "style": "IPY_MODEL_39c05f43f3ee459994f5236ad77faeaa",
            "value": " 160k/160k [00:00&lt;00:00, 1.90MB/s]"
          }
        },
        "2a94114405d44f6ba0f77086bdbfc0f3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "da6a5ca824b04a1b9857e0c6def3c7cd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bd803f6527d5418496852c5a6beb8ba8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e657029ec8fe43db8aee61d6aff71a6b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "87e9c605ef9c47ef8e293ba2b18199b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d2eacc640f764a08abecc15060286ac9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "39c05f43f3ee459994f5236ad77faeaa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "22292d81b1674adb92e0c072ea4cfcb9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_56251e26005d4ac6b0e04af5fd0e60c7",
              "IPY_MODEL_fd72c08d6e984d4284d75f2cb3f0c46a",
              "IPY_MODEL_43fc31fd10634c77a5210b4fa4cce09b"
            ],
            "layout": "IPY_MODEL_da0be6f05cf1490bb5f89ef35f7973b6"
          }
        },
        "56251e26005d4ac6b0e04af5fd0e60c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5629ce20bb51417b9c8b31e5932bda85",
            "placeholder": "​",
            "style": "IPY_MODEL_0d71d1993be24c1ba9aaf56ac4a92ebb",
            "value": "sentence_bert_config.json: 100%"
          }
        },
        "fd72c08d6e984d4284d75f2cb3f0c46a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5b6bd2eb7e724ea1b5bc9cda05cfbc62",
            "max": 57,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9d700c033a0b48d8bc11aef3ab059c9f",
            "value": 57
          }
        },
        "43fc31fd10634c77a5210b4fa4cce09b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ce12db9351f64f5c93338d0798849804",
            "placeholder": "​",
            "style": "IPY_MODEL_9de02b14240d437796f7a82114b876c6",
            "value": " 57.0/57.0 [00:00&lt;00:00, 4.12kB/s]"
          }
        },
        "da0be6f05cf1490bb5f89ef35f7973b6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5629ce20bb51417b9c8b31e5932bda85": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0d71d1993be24c1ba9aaf56ac4a92ebb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5b6bd2eb7e724ea1b5bc9cda05cfbc62": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9d700c033a0b48d8bc11aef3ab059c9f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ce12db9351f64f5c93338d0798849804": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9de02b14240d437796f7a82114b876c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3e781fd46f174a5390e9ec4938e58028": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7c3787d696a4429eb9fb77eabdfa14d3",
              "IPY_MODEL_869ed1b15660425f9230ea30db9ef93c",
              "IPY_MODEL_ed4e09c614e9460088bcd73775b4a55e"
            ],
            "layout": "IPY_MODEL_25c2748473274e569f19139fbdf768f2"
          }
        },
        "7c3787d696a4429eb9fb77eabdfa14d3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2469d48fda8946c1ab9c34a47f69a65d",
            "placeholder": "​",
            "style": "IPY_MODEL_c1eb08a1239a4a6d9c812fec0bccc839",
            "value": "config.json: 100%"
          }
        },
        "869ed1b15660425f9230ea30db9ef93c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_63661a0931d74faa964d0d24987c902c",
            "max": 690,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8ebbd30140ba452d8b3dcaf31469c1b4",
            "value": 690
          }
        },
        "ed4e09c614e9460088bcd73775b4a55e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_68d2c04eb47246048b99d180db479228",
            "placeholder": "​",
            "style": "IPY_MODEL_da72e6f08bca47e3b9586416a350b751",
            "value": " 690/690 [00:00&lt;00:00, 38.4kB/s]"
          }
        },
        "25c2748473274e569f19139fbdf768f2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2469d48fda8946c1ab9c34a47f69a65d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c1eb08a1239a4a6d9c812fec0bccc839": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "63661a0931d74faa964d0d24987c902c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8ebbd30140ba452d8b3dcaf31469c1b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "68d2c04eb47246048b99d180db479228": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "da72e6f08bca47e3b9586416a350b751": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "911c647cc8214cbd8835965391692084": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_34b5d2a50c254a86a2d2a4ccf9597f5a",
              "IPY_MODEL_89ec7cd5892d4fdf84f223856a9380bb",
              "IPY_MODEL_ba86527ed8534518be5d8d426dee152d"
            ],
            "layout": "IPY_MODEL_c10b4a01c752465f808071a94b60873e"
          }
        },
        "34b5d2a50c254a86a2d2a4ccf9597f5a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6bd66c14b62d4d74947ab89f2019dbe9",
            "placeholder": "​",
            "style": "IPY_MODEL_e7f6a873fc9a44d2bb2fe670d1c10b97",
            "value": "model.safetensors: 100%"
          }
        },
        "89ec7cd5892d4fdf84f223856a9380bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fd9f8de890224ad8bfdace6322fbb855",
            "max": 2239611368,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7474d3b795fc4f4eb7dd2cd51f978436",
            "value": 2239611368
          }
        },
        "ba86527ed8534518be5d8d426dee152d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_91f2d32904ad425db8aa0e35a5257a81",
            "placeholder": "​",
            "style": "IPY_MODEL_60b718ed40164c76ba3c332c445a8438",
            "value": " 2.24G/2.24G [00:15&lt;00:00, 253MB/s]"
          }
        },
        "c10b4a01c752465f808071a94b60873e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6bd66c14b62d4d74947ab89f2019dbe9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e7f6a873fc9a44d2bb2fe670d1c10b97": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fd9f8de890224ad8bfdace6322fbb855": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7474d3b795fc4f4eb7dd2cd51f978436": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "91f2d32904ad425db8aa0e35a5257a81": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "60b718ed40164c76ba3c332c445a8438": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "64f6cc028fd34963834f99124c1fd7e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6664f07e38774d07bd301c0bebb77925",
              "IPY_MODEL_4655dfaf856b4e9385fab329a4309940",
              "IPY_MODEL_de96aebccbb641fa8d7e490b6953f13b"
            ],
            "layout": "IPY_MODEL_f2cf4d86ad3e4238acb2621cb9d7db2e"
          }
        },
        "6664f07e38774d07bd301c0bebb77925": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3b82ec88f6344ecf9e4645bace802d68",
            "placeholder": "​",
            "style": "IPY_MODEL_d37a1d33661e4f6d91b61953e9e91388",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "4655dfaf856b4e9385fab329a4309940": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_927c7d9b689043b4a2f30bdca4171c4a",
            "max": 418,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1fce521471704f3c8a6e23a83815c140",
            "value": 418
          }
        },
        "de96aebccbb641fa8d7e490b6953f13b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d7b8965a616645958a9f85a0627b3760",
            "placeholder": "​",
            "style": "IPY_MODEL_4dd8f6ec6f4845ec99a467a7f728fc9f",
            "value": " 418/418 [00:00&lt;00:00, 21.2kB/s]"
          }
        },
        "f2cf4d86ad3e4238acb2621cb9d7db2e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3b82ec88f6344ecf9e4645bace802d68": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d37a1d33661e4f6d91b61953e9e91388": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "927c7d9b689043b4a2f30bdca4171c4a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1fce521471704f3c8a6e23a83815c140": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d7b8965a616645958a9f85a0627b3760": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4dd8f6ec6f4845ec99a467a7f728fc9f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "53e3c5c639e44cd7b776c17b9168689e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_85a83fce64b14f8b9ad8b1c455f9fadf",
              "IPY_MODEL_f0185fd2199d4d3ca696da403056374a",
              "IPY_MODEL_b28b625583d14e2382605fdbb1d960f7"
            ],
            "layout": "IPY_MODEL_4ff614c930ca488ebb5fdbaa168646f8"
          }
        },
        "85a83fce64b14f8b9ad8b1c455f9fadf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5c65852e09e6474ba076ea639da0c80b",
            "placeholder": "​",
            "style": "IPY_MODEL_ddea5766a7f247039f4c4c17233482aa",
            "value": "sentencepiece.bpe.model: 100%"
          }
        },
        "f0185fd2199d4d3ca696da403056374a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_de1b256950bc4e338c12f22b1ff95db4",
            "max": 5069051,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0850b93a806743e1bd8a5c8afd5da30e",
            "value": 5069051
          }
        },
        "b28b625583d14e2382605fdbb1d960f7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2ec3f51f583c47119a3e18d695f303b7",
            "placeholder": "​",
            "style": "IPY_MODEL_92ea48a0533940b8858275333023a087",
            "value": " 5.07M/5.07M [00:00&lt;00:00, 40.7MB/s]"
          }
        },
        "4ff614c930ca488ebb5fdbaa168646f8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5c65852e09e6474ba076ea639da0c80b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ddea5766a7f247039f4c4c17233482aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "de1b256950bc4e338c12f22b1ff95db4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0850b93a806743e1bd8a5c8afd5da30e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2ec3f51f583c47119a3e18d695f303b7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "92ea48a0533940b8858275333023a087": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "228e225e11824b79bbfd502209dea051": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e76ad20e5e8b4f5593db88271200f7b1",
              "IPY_MODEL_e4dafa1ef17d4d1990b1dbff99d91dfe",
              "IPY_MODEL_1293e9c92c5c44bcb9d9ce1cb85ac5a3"
            ],
            "layout": "IPY_MODEL_ad21d51bc2094427a27cf191c6461c86"
          }
        },
        "e76ad20e5e8b4f5593db88271200f7b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c42c0cc04f64412f89a8049844432cb9",
            "placeholder": "​",
            "style": "IPY_MODEL_8724436f199648b3b21d4f90fea918a6",
            "value": "tokenizer.json: 100%"
          }
        },
        "e4dafa1ef17d4d1990b1dbff99d91dfe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e236467d67924813acd8f8c2c05845ab",
            "max": 17082660,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d5eca3b751794093b6dfa92fe0c5fb7b",
            "value": 17082660
          }
        },
        "1293e9c92c5c44bcb9d9ce1cb85ac5a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c91db06f3a9e4e6b851c11a44657a8f8",
            "placeholder": "​",
            "style": "IPY_MODEL_f08c76fc4fc6478fb28430ff0f9c5880",
            "value": " 17.1M/17.1M [00:00&lt;00:00, 167MB/s]"
          }
        },
        "ad21d51bc2094427a27cf191c6461c86": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c42c0cc04f64412f89a8049844432cb9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8724436f199648b3b21d4f90fea918a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e236467d67924813acd8f8c2c05845ab": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d5eca3b751794093b6dfa92fe0c5fb7b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c91db06f3a9e4e6b851c11a44657a8f8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f08c76fc4fc6478fb28430ff0f9c5880": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b30e35c86fe24e279e82b3cb49b8194d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9413960974484bd29908a3d81799d3f3",
              "IPY_MODEL_fd457e9ce5f941cd878c5fc1bbb32e8c",
              "IPY_MODEL_305ebee899d24acbb438e1d7d612b0ef"
            ],
            "layout": "IPY_MODEL_6fe834022a0e455b878e00a964234036"
          }
        },
        "9413960974484bd29908a3d81799d3f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c711896e79fa4f1894909ed4f938f9df",
            "placeholder": "​",
            "style": "IPY_MODEL_fbe7ddc357444763bbcfcffb97a117f8",
            "value": "special_tokens_map.json: 100%"
          }
        },
        "fd457e9ce5f941cd878c5fc1bbb32e8c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d96837cf42ed490680cea816a8bcbbc5",
            "max": 280,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c1ccd2a9cd884144974db67144025ce8",
            "value": 280
          }
        },
        "305ebee899d24acbb438e1d7d612b0ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f2c98e801a0a40cea860140231b72b22",
            "placeholder": "​",
            "style": "IPY_MODEL_7223c527029640248c9296d0e1b9bbd0",
            "value": " 280/280 [00:00&lt;00:00, 23.1kB/s]"
          }
        },
        "6fe834022a0e455b878e00a964234036": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c711896e79fa4f1894909ed4f938f9df": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fbe7ddc357444763bbcfcffb97a117f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d96837cf42ed490680cea816a8bcbbc5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c1ccd2a9cd884144974db67144025ce8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f2c98e801a0a40cea860140231b72b22": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7223c527029640248c9296d0e1b9bbd0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0ff08f06dd004861a8af18fbbec4e837": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_43cc1fcd83b94e6d992db2723d80c8ea",
              "IPY_MODEL_fe93f10c502e4f67934f03c5c085796d",
              "IPY_MODEL_07c4b529a6474689bd0070376a6283ef"
            ],
            "layout": "IPY_MODEL_9b30eef900d843a2bbf0b397142d4842"
          }
        },
        "43cc1fcd83b94e6d992db2723d80c8ea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5070b1385a8e4c5ab38aac0384ad5f90",
            "placeholder": "​",
            "style": "IPY_MODEL_953d1505ccd14cde83a9729650b00ce4",
            "value": "1_Pooling/config.json: 100%"
          }
        },
        "fe93f10c502e4f67934f03c5c085796d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_907c6ce82b174aefb417368501fb03d2",
            "max": 201,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_fb2dd261383d4781bb35e3022ad7e995",
            "value": 201
          }
        },
        "07c4b529a6474689bd0070376a6283ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bac0b66a5d3c4219b00f85b9e74b811b",
            "placeholder": "​",
            "style": "IPY_MODEL_b856a1647a4d4c658ece93ad1e317430",
            "value": " 201/201 [00:00&lt;00:00, 17.0kB/s]"
          }
        },
        "9b30eef900d843a2bbf0b397142d4842": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5070b1385a8e4c5ab38aac0384ad5f90": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "953d1505ccd14cde83a9729650b00ce4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "907c6ce82b174aefb417368501fb03d2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fb2dd261383d4781bb35e3022ad7e995": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "bac0b66a5d3c4219b00f85b9e74b811b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b856a1647a4d4c658ece93ad1e317430": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/INVISIBLE-SAM/SemEval-2025-Task-11---Track-A/blob/main/ml_based_method_E5_large.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# labse hindi"
      ],
      "metadata": {
        "id": "W1e0DscoSRhx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "785959980235465ead70d5f8d8bc5cfe",
            "44aa3b73963f47a0aacaedb96e9638d5",
            "d667dfef3c8c49d1adc938678437841d",
            "5bc671b6536b4b85abafe8f603c6aca6",
            "a778f61cde804245b4cb768200594ab3",
            "f34bd8ec4e6f4ddc9bdedfb3234812a3",
            "3af8aaa2a5dc4a8490074629830202cd",
            "c3fc6ec4a3e44d1484d66390b01a4d46",
            "b9edf9b85b94489ca9f26948b46f7ad1",
            "9c67d320ee0e405787730154e295cdbb",
            "33fe5d200c4f498abbebd5f5d72957e1",
            "17eecf9e012a46e5ae0744c01d72d9b9",
            "f0abb7ab38ad490da22875912957fcf8",
            "06fc57a36b614d3880295be8f4537bad",
            "6723794779fa4aa18ab1255937f16c32",
            "2a94114405d44f6ba0f77086bdbfc0f3",
            "da6a5ca824b04a1b9857e0c6def3c7cd",
            "bd803f6527d5418496852c5a6beb8ba8",
            "e657029ec8fe43db8aee61d6aff71a6b",
            "87e9c605ef9c47ef8e293ba2b18199b8",
            "d2eacc640f764a08abecc15060286ac9",
            "39c05f43f3ee459994f5236ad77faeaa",
            "22292d81b1674adb92e0c072ea4cfcb9",
            "56251e26005d4ac6b0e04af5fd0e60c7",
            "fd72c08d6e984d4284d75f2cb3f0c46a",
            "43fc31fd10634c77a5210b4fa4cce09b",
            "da0be6f05cf1490bb5f89ef35f7973b6",
            "5629ce20bb51417b9c8b31e5932bda85",
            "0d71d1993be24c1ba9aaf56ac4a92ebb",
            "5b6bd2eb7e724ea1b5bc9cda05cfbc62",
            "9d700c033a0b48d8bc11aef3ab059c9f",
            "ce12db9351f64f5c93338d0798849804",
            "9de02b14240d437796f7a82114b876c6",
            "3e781fd46f174a5390e9ec4938e58028",
            "7c3787d696a4429eb9fb77eabdfa14d3",
            "869ed1b15660425f9230ea30db9ef93c",
            "ed4e09c614e9460088bcd73775b4a55e",
            "25c2748473274e569f19139fbdf768f2",
            "2469d48fda8946c1ab9c34a47f69a65d",
            "c1eb08a1239a4a6d9c812fec0bccc839",
            "63661a0931d74faa964d0d24987c902c",
            "8ebbd30140ba452d8b3dcaf31469c1b4",
            "68d2c04eb47246048b99d180db479228",
            "da72e6f08bca47e3b9586416a350b751",
            "911c647cc8214cbd8835965391692084",
            "34b5d2a50c254a86a2d2a4ccf9597f5a",
            "89ec7cd5892d4fdf84f223856a9380bb",
            "ba86527ed8534518be5d8d426dee152d",
            "c10b4a01c752465f808071a94b60873e",
            "6bd66c14b62d4d74947ab89f2019dbe9",
            "e7f6a873fc9a44d2bb2fe670d1c10b97",
            "fd9f8de890224ad8bfdace6322fbb855",
            "7474d3b795fc4f4eb7dd2cd51f978436",
            "91f2d32904ad425db8aa0e35a5257a81",
            "60b718ed40164c76ba3c332c445a8438",
            "64f6cc028fd34963834f99124c1fd7e7",
            "6664f07e38774d07bd301c0bebb77925",
            "4655dfaf856b4e9385fab329a4309940",
            "de96aebccbb641fa8d7e490b6953f13b",
            "f2cf4d86ad3e4238acb2621cb9d7db2e",
            "3b82ec88f6344ecf9e4645bace802d68",
            "d37a1d33661e4f6d91b61953e9e91388",
            "927c7d9b689043b4a2f30bdca4171c4a",
            "1fce521471704f3c8a6e23a83815c140",
            "d7b8965a616645958a9f85a0627b3760",
            "4dd8f6ec6f4845ec99a467a7f728fc9f",
            "53e3c5c639e44cd7b776c17b9168689e",
            "85a83fce64b14f8b9ad8b1c455f9fadf",
            "f0185fd2199d4d3ca696da403056374a",
            "b28b625583d14e2382605fdbb1d960f7",
            "4ff614c930ca488ebb5fdbaa168646f8",
            "5c65852e09e6474ba076ea639da0c80b",
            "ddea5766a7f247039f4c4c17233482aa",
            "de1b256950bc4e338c12f22b1ff95db4",
            "0850b93a806743e1bd8a5c8afd5da30e",
            "2ec3f51f583c47119a3e18d695f303b7",
            "92ea48a0533940b8858275333023a087",
            "228e225e11824b79bbfd502209dea051",
            "e76ad20e5e8b4f5593db88271200f7b1",
            "e4dafa1ef17d4d1990b1dbff99d91dfe",
            "1293e9c92c5c44bcb9d9ce1cb85ac5a3",
            "ad21d51bc2094427a27cf191c6461c86",
            "c42c0cc04f64412f89a8049844432cb9",
            "8724436f199648b3b21d4f90fea918a6",
            "e236467d67924813acd8f8c2c05845ab",
            "d5eca3b751794093b6dfa92fe0c5fb7b",
            "c91db06f3a9e4e6b851c11a44657a8f8",
            "f08c76fc4fc6478fb28430ff0f9c5880",
            "b30e35c86fe24e279e82b3cb49b8194d",
            "9413960974484bd29908a3d81799d3f3",
            "fd457e9ce5f941cd878c5fc1bbb32e8c",
            "305ebee899d24acbb438e1d7d612b0ef",
            "6fe834022a0e455b878e00a964234036",
            "c711896e79fa4f1894909ed4f938f9df",
            "fbe7ddc357444763bbcfcffb97a117f8",
            "d96837cf42ed490680cea816a8bcbbc5",
            "c1ccd2a9cd884144974db67144025ce8",
            "f2c98e801a0a40cea860140231b72b22",
            "7223c527029640248c9296d0e1b9bbd0",
            "0ff08f06dd004861a8af18fbbec4e837",
            "43cc1fcd83b94e6d992db2723d80c8ea",
            "fe93f10c502e4f67934f03c5c085796d",
            "07c4b529a6474689bd0070376a6283ef",
            "9b30eef900d843a2bbf0b397142d4842",
            "5070b1385a8e4c5ab38aac0384ad5f90",
            "953d1505ccd14cde83a9729650b00ce4",
            "907c6ce82b174aefb417368501fb03d2",
            "fb2dd261383d4781bb35e3022ad7e995",
            "bac0b66a5d3c4219b00f85b9e74b811b",
            "b856a1647a4d4c658ece93ad1e317430"
          ]
        },
        "id": "KWlE0-D-Rwc9",
        "outputId": "397a0ca3-e34b-4481-b773-5e5ea061ae1c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "modules.json:   0%|          | 0.00/387 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "785959980235465ead70d5f8d8bc5cfe"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md:   0%|          | 0.00/160k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "17eecf9e012a46e5ae0744c01d72d9b9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "sentence_bert_config.json:   0%|          | 0.00/57.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "22292d81b1674adb92e0c072ea4cfcb9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/690 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3e781fd46f174a5390e9ec4938e58028"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/2.24G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "911c647cc8214cbd8835965391692084"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/418 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "64f6cc028fd34963834f99124c1fd7e7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "53e3c5c639e44cd7b776c17b9168689e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/17.1M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "228e225e11824b79bbfd502209dea051"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/280 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b30e35c86fe24e279e82b3cb49b8194d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "1_Pooling/config.json:   0%|          | 0.00/201 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0ff08f06dd004861a8af18fbbec4e837"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==============================================\n",
            "Training SVM...\n",
            "Evaluation on Train Set:\n",
            "\n",
            "Model: SVM - Train Set\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       anger       0.99      0.93      0.96       422\n",
            "     disgust       0.98      0.91      0.95       265\n",
            "        fear       0.99      0.93      0.96       380\n",
            "         joy       1.00      0.98      0.99       442\n",
            "     sadness       0.99      0.91      0.95       449\n",
            "    surprise       1.00      0.98      0.99       278\n",
            "\n",
            "   micro avg       0.99      0.94      0.96      2236\n",
            "   macro avg       0.99      0.94      0.96      2236\n",
            "weighted avg       0.99      0.94      0.96      2236\n",
            " samples avg       0.74      0.73      0.73      2236\n",
            "\n",
            "Macro F1 Score for Train Set: 0.9646813168740239\n",
            "\n",
            "Evaluation on Test Set:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model: SVM - Test Set\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       anger       0.86      0.75      0.80        16\n",
            "     disgust       0.86      0.60      0.71        10\n",
            "        fear       1.00      0.86      0.92        14\n",
            "         joy       0.75      0.82      0.78        11\n",
            "     sadness       0.80      0.47      0.59        17\n",
            "    surprise       0.88      0.78      0.82         9\n",
            "\n",
            "   micro avg       0.86      0.70      0.77        77\n",
            "   macro avg       0.86      0.71      0.77        77\n",
            "weighted avg       0.86      0.70      0.76        77\n",
            " samples avg       0.50      0.50      0.49        77\n",
            "\n",
            "Macro F1 Score for Test Set: 0.771281662671262\n",
            "\n",
            "Summary for SVM:\n",
            "  Train Macro F1 Score: 0.9646813168740239\n",
            "  Test Macro F1 Score:  0.771281662671262\n",
            "==============================================\n",
            "\n",
            "==============================================\n",
            "Training Naive Bayes...\n",
            "Evaluation on Train Set:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model: Naive Bayes - Train Set\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       anger       0.51      0.93      0.66       422\n",
            "     disgust       0.48      0.91      0.63       265\n",
            "        fear       0.62      0.86      0.72       380\n",
            "         joy       0.77      0.95      0.85       442\n",
            "     sadness       0.49      0.87      0.63       449\n",
            "    surprise       0.61      0.95      0.74       278\n",
            "\n",
            "   micro avg       0.57      0.91      0.70      2236\n",
            "   macro avg       0.58      0.91      0.70      2236\n",
            "weighted avg       0.58      0.91      0.71      2236\n",
            " samples avg       0.50      0.70      0.56      2236\n",
            "\n",
            "Macro F1 Score for Train Set: 0.7046460585351668\n",
            "\n",
            "Evaluation on Test Set:\n",
            "\n",
            "Model: Naive Bayes - Test Set\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       anger       0.48      0.81      0.60        16\n",
            "     disgust       0.50      0.90      0.64        10\n",
            "        fear       0.67      1.00      0.80        14\n",
            "         joy       0.58      1.00      0.73        11\n",
            "     sadness       0.45      0.82      0.58        17\n",
            "    surprise       0.47      0.89      0.62         9\n",
            "\n",
            "   micro avg       0.52      0.90      0.66        77\n",
            "   macro avg       0.52      0.90      0.66        77\n",
            "weighted avg       0.52      0.90      0.66        77\n",
            " samples avg       0.40      0.62      0.47        77\n",
            "\n",
            "Macro F1 Score for Test Set: 0.6632599312831872\n",
            "\n",
            "Summary for Naive Bayes:\n",
            "  Train Macro F1 Score: 0.7046460585351668\n",
            "  Test Macro F1 Score:  0.6632599312831872\n",
            "==============================================\n",
            "\n",
            "==============================================\n",
            "Training Logistic Regression...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation on Train Set:\n",
            "\n",
            "Model: Logistic Regression - Train Set\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       anger       1.00      1.00      1.00       422\n",
            "     disgust       1.00      1.00      1.00       265\n",
            "        fear       1.00      1.00      1.00       380\n",
            "         joy       1.00      1.00      1.00       442\n",
            "     sadness       1.00      1.00      1.00       449\n",
            "    surprise       1.00      1.00      1.00       278\n",
            "\n",
            "   micro avg       1.00      1.00      1.00      2236\n",
            "   macro avg       1.00      1.00      1.00      2236\n",
            "weighted avg       1.00      1.00      1.00      2236\n",
            " samples avg       0.76      0.76      0.76      2236\n",
            "\n",
            "Macro F1 Score for Train Set: 1.0\n",
            "\n",
            "Evaluation on Test Set:\n",
            "\n",
            "Model: Logistic Regression - Test Set\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       anger       0.81      0.81      0.81        16\n",
            "     disgust       0.70      0.70      0.70        10\n",
            "        fear       0.92      0.86      0.89        14\n",
            "         joy       0.67      0.73      0.70        11\n",
            "     sadness       0.67      0.71      0.69        17\n",
            "    surprise       0.90      1.00      0.95         9\n",
            "\n",
            "   micro avg       0.77      0.79      0.78        77\n",
            "   macro avg       0.78      0.80      0.79        77\n",
            "weighted avg       0.78      0.79      0.78        77\n",
            " samples avg       0.52      0.56      0.53        77\n",
            "\n",
            "Macro F1 Score for Test Set: 0.7883539615948082\n",
            "\n",
            "Summary for Logistic Regression:\n",
            "  Train Macro F1 Score: 1.0\n",
            "  Test Macro F1 Score:  0.7883539615948082\n",
            "==============================================\n",
            "\n",
            "==============================================\n",
            "Training Random Forest...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation on Train Set:\n",
            "\n",
            "Model: Random Forest - Train Set\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       anger       1.00      0.95      0.98       422\n",
            "     disgust       1.00      0.86      0.92       265\n",
            "        fear       1.00      0.91      0.95       380\n",
            "         joy       1.00      0.98      0.99       442\n",
            "     sadness       1.00      0.94      0.97       449\n",
            "    surprise       1.00      0.92      0.96       278\n",
            "\n",
            "   micro avg       1.00      0.93      0.97      2236\n",
            "   macro avg       1.00      0.93      0.96      2236\n",
            "weighted avg       1.00      0.93      0.97      2236\n",
            " samples avg       0.73      0.72      0.72      2236\n",
            "\n",
            "Macro F1 Score for Train Set: 0.9620522251838709\n",
            "\n",
            "Evaluation on Test Set:\n",
            "\n",
            "Model: Random Forest - Test Set\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       anger       1.00      0.06      0.12        16\n",
            "     disgust       1.00      0.20      0.33        10\n",
            "        fear       1.00      0.36      0.53        14\n",
            "         joy       0.80      0.36      0.50        11\n",
            "     sadness       1.00      0.18      0.30        17\n",
            "    surprise       1.00      0.33      0.50         9\n",
            "\n",
            "   micro avg       0.95      0.23      0.38        77\n",
            "   macro avg       0.97      0.25      0.38        77\n",
            "weighted avg       0.97      0.23      0.36        77\n",
            " samples avg       0.17      0.17      0.17        77\n",
            "\n",
            "Macro F1 Score for Test Set: 0.3795493636050911\n",
            "\n",
            "Summary for Random Forest:\n",
            "  Train Macro F1 Score: 0.9620522251838709\n",
            "  Test Macro F1 Score:  0.3795493636050911\n",
            "==============================================\n",
            "\n",
            "Example prediction using SVM:\n",
            "anger: 0\n",
            "disgust: 0\n",
            "fear: 0\n",
            "joy: 0\n",
            "sadness: 0\n",
            "surprise: 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Import classifiers and wrappers\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.multioutput import MultiOutputClassifier\n",
        "\n",
        "# Evaluation metrics\n",
        "from sklearn.metrics import classification_report, f1_score\n",
        "\n",
        "# For generating text embeddings\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# For scaling the embeddings\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# -------------------------------\n",
        "# 1. Load Data and Generate Embeddings\n",
        "# -------------------------------\n",
        "\n",
        "# Load datasets\n",
        "train_df = pd.read_csv(\"/content/hin.csv\")\n",
        "test_df = pd.read_csv(\"/content/hin_dev.csv\")\n",
        "\n",
        "# Load the LaBSE model for generating embeddings\n",
        "labse_model = SentenceTransformer('intfloat/multilingual-e5-large')\n",
        "\n",
        "def generate_embeddings(texts):\n",
        "    \"\"\"\n",
        "    Given a list or array of texts, generate and return their normalized embeddings.\n",
        "    \"\"\"\n",
        "    return np.array(labse_model.encode(texts, convert_to_numpy=True, normalize_embeddings=True))\n",
        "\n",
        "# Get text data\n",
        "train_texts = train_df['text'].values\n",
        "test_texts = test_df['text'].values\n",
        "\n",
        "# Generate embeddings for train and test sets\n",
        "train_embeddings = generate_embeddings(train_texts)\n",
        "test_embeddings = generate_embeddings(test_texts)\n",
        "\n",
        "# -------------------------------\n",
        "# 2. Prepare Labels and Scale Features\n",
        "# -------------------------------\n",
        "\n",
        "# Define the emotion label columns\n",
        "label_columns = ['anger', 'disgust', 'fear', 'joy', 'sadness', 'surprise']\n",
        "\n",
        "# Extract label arrays\n",
        "train_labels = train_df[label_columns].values\n",
        "test_labels = test_df[label_columns].values\n",
        "\n",
        "# Scale the embeddings (feature standardization)\n",
        "scaler = StandardScaler()\n",
        "train_embeddings_scaled = scaler.fit_transform(train_embeddings)\n",
        "test_embeddings_scaled = scaler.transform(test_embeddings)\n",
        "\n",
        "# -------------------------------\n",
        "# 3. Define, Train, and Evaluate Models\n",
        "# -------------------------------\n",
        "\n",
        "# Define the four models. For Random Forest we add some regularization\n",
        "# by limiting the maximum tree depth and controlling the minimum samples needed for splits.\n",
        "models = {\n",
        "    \"SVM\": MultiOutputClassifier(SVC(kernel='rbf', probability=True, random_state=42)),\n",
        "    \"Naive Bayes\": MultiOutputClassifier(GaussianNB()),\n",
        "    \"Logistic Regression\": MultiOutputClassifier(\n",
        "        LogisticRegression(max_iter=1000, random_state=42)\n",
        "    ),\n",
        "    \"Random Forest\": MultiOutputClassifier(\n",
        "        RandomForestClassifier(max_depth=10, min_samples_split=5, random_state=42)\n",
        "    )\n",
        "}\n",
        "\n",
        "def evaluate_model(model, embeddings, labels, dataset_name, model_name):\n",
        "    \"\"\"\n",
        "    Predict labels using the given model and embeddings, then print the classification report\n",
        "    and macro F1 score.\n",
        "    \"\"\"\n",
        "    predictions = model.predict(embeddings)\n",
        "    print(f\"\\nModel: {model_name} - {dataset_name}\")\n",
        "    print(\"Classification Report:\")\n",
        "    print(classification_report(labels, predictions, target_names=label_columns))\n",
        "\n",
        "    macro_f1 = f1_score(labels, predictions, average='macro')\n",
        "    print(f\"Macro F1 Score for {dataset_name}: {macro_f1}\\n\")\n",
        "    return macro_f1\n",
        "\n",
        "# Train and evaluate each model on both the training and testing sets.\n",
        "for model_name, clf in models.items():\n",
        "    print(\"==============================================\")\n",
        "    print(f\"Training {model_name}...\")\n",
        "    clf.fit(train_embeddings_scaled, train_labels)\n",
        "\n",
        "    # Evaluate on the Training Set\n",
        "    print(\"Evaluation on Train Set:\")\n",
        "    train_macro_f1 = evaluate_model(clf, train_embeddings_scaled, train_labels, \"Train Set\", model_name)\n",
        "\n",
        "    # Evaluate on the Testing Set\n",
        "    print(\"Evaluation on Test Set:\")\n",
        "    test_macro_f1 = evaluate_model(clf, test_embeddings_scaled, test_labels, \"Test Set\", model_name)\n",
        "\n",
        "    print(f\"Summary for {model_name}:\")\n",
        "    print(f\"  Train Macro F1 Score: {train_macro_f1}\")\n",
        "    print(f\"  Test Macro F1 Score:  {test_macro_f1}\")\n",
        "    print(\"==============================================\\n\")\n",
        "\n",
        "# -------------------------------\n",
        "# 4. Predict Emotions for New Text\n",
        "# -------------------------------\n",
        "\n",
        "def predict_emotions(text, model, scaler):\n",
        "    \"\"\"\n",
        "    Given a new text string, generate its embedding, scale it, and predict emotion labels.\n",
        "    Returns a dictionary mapping each emotion to its predicted label (0 or 1).\n",
        "    \"\"\"\n",
        "    # Generate embedding for the new text\n",
        "    embedding = generate_embeddings([text])\n",
        "    embedding_scaled = scaler.transform(embedding)\n",
        "\n",
        "    # Get predictions\n",
        "    predictions = model.predict(embedding_scaled)\n",
        "\n",
        "    # Map each label to its corresponding emotion\n",
        "    emotion_preds = {emotion: int(pred) for emotion, pred in zip(label_columns, predictions[0])}\n",
        "    return emotion_preds\n",
        "\n",
        "# Example usage for prediction using the SVM model (you can choose any of the trained models)\n",
        "example_text = \"यह एक उदाहरण वाक्य है\"\n",
        "print(\"Example prediction using SVM:\")\n",
        "svm_model = models[\"SVM\"]\n",
        "example_prediction = predict_emotions(example_text, svm_model, scaler)\n",
        "for emotion, pred in example_prediction.items():\n",
        "    print(f\"{emotion}: {pred}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Amharic"
      ],
      "metadata": {
        "id": "SPqXcg_tUc5Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Import classifiers and wrappers\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.multioutput import MultiOutputClassifier\n",
        "\n",
        "# Evaluation metrics\n",
        "from sklearn.metrics import classification_report, f1_score\n",
        "\n",
        "# For generating text embeddings\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# For scaling the embeddings\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# -------------------------------\n",
        "# 1. Load Data and Generate Embeddings\n",
        "# -------------------------------\n",
        "\n",
        "# Load datasets\n",
        "train_df = pd.read_csv(\"/content/amh.csv\")\n",
        "test_df = pd.read_csv(\"/content/amh_dev.csv\")\n",
        "\n",
        "# Load the LaBSE model for generating embeddings\n",
        "labse_model = SentenceTransformer('intfloat/multilingual-e5-large')\n",
        "\n",
        "def generate_embeddings(texts):\n",
        "    \"\"\"\n",
        "    Given a list or array of texts, generate and return their normalized embeddings.\n",
        "    \"\"\"\n",
        "    return np.array(labse_model.encode(texts, convert_to_numpy=True, normalize_embeddings=True))\n",
        "\n",
        "# Get text data\n",
        "train_texts = train_df['text'].values\n",
        "test_texts = test_df['text'].values\n",
        "\n",
        "# Generate embeddings for train and test sets\n",
        "train_embeddings = generate_embeddings(train_texts)\n",
        "test_embeddings = generate_embeddings(test_texts)\n",
        "\n",
        "# -------------------------------\n",
        "# 2. Prepare Labels and Scale Features\n",
        "# -------------------------------\n",
        "\n",
        "# Define the emotion label columns\n",
        "label_columns = ['anger', 'disgust', 'fear', 'joy', 'sadness', 'surprise']\n",
        "\n",
        "# Extract label arrays\n",
        "train_labels = train_df[label_columns].values\n",
        "test_labels = test_df[label_columns].values\n",
        "\n",
        "# Scale the embeddings (feature standardization)\n",
        "scaler = StandardScaler()\n",
        "train_embeddings_scaled = scaler.fit_transform(train_embeddings)\n",
        "test_embeddings_scaled = scaler.transform(test_embeddings)\n",
        "\n",
        "# -------------------------------\n",
        "# 3. Define, Train, and Evaluate Models\n",
        "# -------------------------------\n",
        "\n",
        "# Define the four models. For Random Forest we add some regularization\n",
        "# by limiting the maximum tree depth and controlling the minimum samples needed for splits.\n",
        "models = {\n",
        "    \"SVM\": MultiOutputClassifier(SVC(kernel='rbf', probability=True, random_state=42)),\n",
        "    \"Naive Bayes\": MultiOutputClassifier(GaussianNB()),\n",
        "    \"Logistic Regression\": MultiOutputClassifier(\n",
        "        LogisticRegression(max_iter=1000, random_state=42)\n",
        "    ),\n",
        "    \"Random Forest\": MultiOutputClassifier(\n",
        "        RandomForestClassifier(max_depth=10, min_samples_split=5, random_state=42)\n",
        "    )\n",
        "}\n",
        "\n",
        "def evaluate_model(model, embeddings, labels, dataset_name, model_name):\n",
        "    \"\"\"\n",
        "    Predict labels using the given model and embeddings, then print the classification report\n",
        "    and macro F1 score.\n",
        "    \"\"\"\n",
        "    predictions = model.predict(embeddings)\n",
        "    print(f\"\\nModel: {model_name} - {dataset_name}\")\n",
        "    print(\"Classification Report:\")\n",
        "    print(classification_report(labels, predictions, target_names=label_columns))\n",
        "\n",
        "    macro_f1 = f1_score(labels, predictions, average='macro')\n",
        "    print(f\"Macro F1 Score for {dataset_name}: {macro_f1}\\n\")\n",
        "    return macro_f1\n",
        "\n",
        "# Train and evaluate each model on both the training and testing sets.\n",
        "for model_name, clf in models.items():\n",
        "    print(\"==============================================\")\n",
        "    print(f\"Training {model_name}...\")\n",
        "    clf.fit(train_embeddings_scaled, train_labels)\n",
        "\n",
        "    # Evaluate on the Training Set\n",
        "    print(\"Evaluation on Train Set:\")\n",
        "    train_macro_f1 = evaluate_model(clf, train_embeddings_scaled, train_labels, \"Train Set\", model_name)\n",
        "\n",
        "    # Evaluate on the Testing Set\n",
        "    print(\"Evaluation on Test Set:\")\n",
        "    test_macro_f1 = evaluate_model(clf, test_embeddings_scaled, test_labels, \"Test Set\", model_name)\n",
        "\n",
        "    print(f\"Summary for {model_name}:\")\n",
        "    print(f\"  Train Macro F1 Score: {train_macro_f1}\")\n",
        "    print(f\"  Test Macro F1 Score:  {test_macro_f1}\")\n",
        "    print(\"==============================================\\n\")\n",
        "\n",
        "# -------------------------------\n",
        "# 4. Predict Emotions for New Text\n",
        "# -------------------------------\n",
        "\n",
        "def predict_emotions(text, model, scaler):\n",
        "    \"\"\"\n",
        "    Given a new text string, generate its embedding, scale it, and predict emotion labels.\n",
        "    Returns a dictionary mapping each emotion to its predicted label (0 or 1).\n",
        "    \"\"\"\n",
        "    # Generate embedding for the new text\n",
        "    embedding = generate_embeddings([text])\n",
        "    embedding_scaled = scaler.transform(embedding)\n",
        "\n",
        "    # Get predictions\n",
        "    predictions = model.predict(embedding_scaled)\n",
        "\n",
        "    # Map each label to its corresponding emotion\n",
        "    emotion_preds = {emotion: int(pred) for emotion, pred in zip(label_columns, predictions[0])}\n",
        "    return emotion_preds\n",
        "\n",
        "# Example usage for prediction using the SVM model (you can choose any of the trained models)\n",
        "example_text = \"यह एक उदाहरण वाक्य है\"\n",
        "print(\"Example prediction using SVM:\")\n",
        "svm_model = models[\"SVM\"]\n",
        "example_prediction = predict_emotions(example_text, svm_model, scaler)\n",
        "for emotion, pred in example_prediction.items():\n",
        "    print(f\"{emotion}: {pred}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9I6qAQk5UhVh",
        "outputId": "01a9e29d-13cb-4d23-e417-906dee9c22e3"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==============================================\n",
            "Training SVM...\n",
            "Evaluation on Train Set:\n",
            "\n",
            "Model: SVM - Train Set\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       anger       0.94      0.89      0.92      1188\n",
            "     disgust       0.97      0.90      0.93      1268\n",
            "        fear       1.00      0.27      0.42       109\n",
            "         joy       1.00      0.83      0.91       549\n",
            "     sadness       1.00      0.79      0.88       771\n",
            "    surprise       1.00      0.52      0.69       151\n",
            "\n",
            "   micro avg       0.97      0.84      0.90      4036\n",
            "   macro avg       0.98      0.70      0.79      4036\n",
            "weighted avg       0.97      0.84      0.89      4036\n",
            " samples avg       0.72      0.70      0.70      4036\n",
            "\n",
            "Macro F1 Score for Train Set: 0.7917302619300876\n",
            "\n",
            "Evaluation on Test Set:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model: SVM - Test Set\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       anger       0.66      0.56      0.60       207\n",
            "     disgust       0.77      0.64      0.70       209\n",
            "        fear       0.00      0.00      0.00        22\n",
            "         joy       0.84      0.45      0.59        93\n",
            "     sadness       0.83      0.31      0.46       127\n",
            "    surprise       1.00      0.04      0.07        27\n",
            "\n",
            "   micro avg       0.74      0.48      0.59       685\n",
            "   macro avg       0.68      0.33      0.40       685\n",
            "weighted avg       0.74      0.48      0.56       685\n",
            " samples avg       0.43      0.42      0.41       685\n",
            "\n",
            "Macro F1 Score for Test Set: 0.4027476268641191\n",
            "\n",
            "Summary for SVM:\n",
            "  Train Macro F1 Score: 0.7917302619300876\n",
            "  Test Macro F1 Score:  0.4027476268641191\n",
            "==============================================\n",
            "\n",
            "==============================================\n",
            "Training Naive Bayes...\n",
            "Evaluation on Train Set:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model: Naive Bayes - Train Set\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       anger       0.59      0.79      0.67      1188\n",
            "     disgust       0.63      0.78      0.70      1268\n",
            "        fear       0.28      0.79      0.41       109\n",
            "         joy       0.51      0.81      0.63       549\n",
            "     sadness       0.50      0.70      0.58       771\n",
            "    surprise       0.30      0.83      0.44       151\n",
            "\n",
            "   micro avg       0.54      0.77      0.63      4036\n",
            "   macro avg       0.47      0.78      0.57      4036\n",
            "weighted avg       0.56      0.77      0.64      4036\n",
            " samples avg       0.46      0.63      0.51      4036\n",
            "\n",
            "Macro F1 Score for Train Set: 0.5728842698042392\n",
            "\n",
            "Evaluation on Test Set:\n",
            "\n",
            "Model: Naive Bayes - Test Set\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       anger       0.55      0.73      0.63       207\n",
            "     disgust       0.61      0.77      0.68       209\n",
            "        fear       0.25      0.59      0.35        22\n",
            "         joy       0.49      0.71      0.58        93\n",
            "     sadness       0.48      0.71      0.57       127\n",
            "    surprise       0.27      0.67      0.38        27\n",
            "\n",
            "   micro avg       0.51      0.73      0.60       685\n",
            "   macro avg       0.44      0.70      0.53       685\n",
            "weighted avg       0.53      0.73      0.61       685\n",
            " samples avg       0.45      0.62      0.49       685\n",
            "\n",
            "Macro F1 Score for Test Set: 0.5318467845838252\n",
            "\n",
            "Summary for Naive Bayes:\n",
            "  Train Macro F1 Score: 0.5728842698042392\n",
            "  Test Macro F1 Score:  0.5318467845838252\n",
            "==============================================\n",
            "\n",
            "==============================================\n",
            "Training Logistic Regression...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation on Train Set:\n",
            "\n",
            "Model: Logistic Regression - Train Set\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       anger       0.85      0.82      0.83      1188\n",
            "     disgust       0.89      0.86      0.88      1268\n",
            "        fear       1.00      1.00      1.00       109\n",
            "         joy       1.00      0.99      1.00       549\n",
            "     sadness       0.94      0.88      0.91       771\n",
            "    surprise       1.00      1.00      1.00       151\n",
            "\n",
            "   micro avg       0.91      0.88      0.89      4036\n",
            "   macro avg       0.95      0.93      0.94      4036\n",
            "weighted avg       0.91      0.88      0.89      4036\n",
            " samples avg       0.73      0.73      0.72      4036\n",
            "\n",
            "Macro F1 Score for Train Set: 0.9357772883703337\n",
            "\n",
            "Evaluation on Test Set:\n",
            "\n",
            "Model: Logistic Regression - Test Set\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       anger       0.59      0.59      0.59       207\n",
            "     disgust       0.61      0.65      0.63       209\n",
            "        fear       0.62      0.45      0.53        22\n",
            "         joy       0.61      0.62      0.62        93\n",
            "     sadness       0.59      0.52      0.55       127\n",
            "    surprise       0.45      0.33      0.38        27\n",
            "\n",
            "   micro avg       0.60      0.59      0.59       685\n",
            "   macro avg       0.58      0.53      0.55       685\n",
            "weighted avg       0.59      0.59      0.59       685\n",
            " samples avg       0.48      0.50      0.47       685\n",
            "\n",
            "Macro F1 Score for Test Set: 0.5496450292156916\n",
            "\n",
            "Summary for Logistic Regression:\n",
            "  Train Macro F1 Score: 0.9357772883703337\n",
            "  Test Macro F1 Score:  0.5496450292156916\n",
            "==============================================\n",
            "\n",
            "==============================================\n",
            "Training Random Forest...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation on Train Set:\n",
            "\n",
            "Model: Random Forest - Train Set\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       anger       1.00      0.99      0.99      1188\n",
            "     disgust       1.00      0.99      0.99      1268\n",
            "        fear       1.00      0.04      0.07       109\n",
            "         joy       1.00      0.77      0.87       549\n",
            "     sadness       1.00      0.79      0.88       771\n",
            "    surprise       1.00      0.11      0.19       151\n",
            "\n",
            "   micro avg       1.00      0.86      0.93      4036\n",
            "   macro avg       1.00      0.61      0.67      4036\n",
            "weighted avg       1.00      0.86      0.90      4036\n",
            " samples avg       0.75      0.71      0.72      4036\n",
            "\n",
            "Macro F1 Score for Train Set: 0.6670336382893183\n",
            "\n",
            "Evaluation on Test Set:\n",
            "\n",
            "Model: Random Forest - Test Set\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       anger       0.71      0.31      0.43       207\n",
            "     disgust       0.84      0.38      0.53       209\n",
            "        fear       0.00      0.00      0.00        22\n",
            "         joy       0.90      0.10      0.17        93\n",
            "     sadness       0.83      0.04      0.08       127\n",
            "    surprise       0.00      0.00      0.00        27\n",
            "\n",
            "   micro avg       0.78      0.23      0.36       685\n",
            "   macro avg       0.55      0.14      0.20       685\n",
            "weighted avg       0.75      0.23      0.33       685\n",
            " samples avg       0.20      0.18      0.19       685\n",
            "\n",
            "Macro F1 Score for Test Set: 0.20184060827459108\n",
            "\n",
            "Summary for Random Forest:\n",
            "  Train Macro F1 Score: 0.6670336382893183\n",
            "  Test Macro F1 Score:  0.20184060827459108\n",
            "==============================================\n",
            "\n",
            "Example prediction using SVM:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "anger: 0\n",
            "disgust: 0\n",
            "fear: 0\n",
            "joy: 0\n",
            "sadness: 0\n",
            "surprise: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# arabic"
      ],
      "metadata": {
        "id": "CLgCBxp-UtNN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Import classifiers and wrappers\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.multioutput import MultiOutputClassifier\n",
        "\n",
        "# Evaluation metrics\n",
        "from sklearn.metrics import classification_report, f1_score\n",
        "\n",
        "# For generating text embeddings\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# For scaling the embeddings\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# -------------------------------\n",
        "# 1. Load Data and Generate Embeddings\n",
        "# -------------------------------\n",
        "\n",
        "# Load datasets\n",
        "train_df = pd.read_csv(\"/content/ary.csv\")\n",
        "test_df = pd.read_csv(\"/content/ary_dev.csv\")\n",
        "\n",
        "# Load the LaBSE model for generating embeddings\n",
        "labse_model = SentenceTransformer('intfloat/multilingual-e5-large')\n",
        "\n",
        "def generate_embeddings(texts):\n",
        "    \"\"\"\n",
        "    Given a list or array of texts, generate and return their normalized embeddings.\n",
        "    \"\"\"\n",
        "    return np.array(labse_model.encode(texts, convert_to_numpy=True, normalize_embeddings=True))\n",
        "\n",
        "# Get text data\n",
        "train_texts = train_df['text'].values\n",
        "test_texts = test_df['text'].values\n",
        "\n",
        "# Generate embeddings for train and test sets\n",
        "train_embeddings = generate_embeddings(train_texts)\n",
        "test_embeddings = generate_embeddings(test_texts)\n",
        "\n",
        "# -------------------------------\n",
        "# 2. Prepare Labels and Scale Features\n",
        "# -------------------------------\n",
        "\n",
        "# Define the emotion label columns\n",
        "label_columns = ['anger', 'disgust', 'fear', 'joy', 'sadness', 'surprise']\n",
        "\n",
        "# Extract label arrays\n",
        "train_labels = train_df[label_columns].values\n",
        "test_labels = test_df[label_columns].values\n",
        "\n",
        "# Scale the embeddings (feature standardization)\n",
        "scaler = StandardScaler()\n",
        "train_embeddings_scaled = scaler.fit_transform(train_embeddings)\n",
        "test_embeddings_scaled = scaler.transform(test_embeddings)\n",
        "\n",
        "# -------------------------------\n",
        "# 3. Define, Train, and Evaluate Models\n",
        "# -------------------------------\n",
        "\n",
        "# Define the four models. For Random Forest we add some regularization\n",
        "# by limiting the maximum tree depth and controlling the minimum samples needed for splits.\n",
        "models = {\n",
        "    \"SVM\": MultiOutputClassifier(SVC(kernel='rbf', probability=True, random_state=42)),\n",
        "    \"Naive Bayes\": MultiOutputClassifier(GaussianNB()),\n",
        "    \"Logistic Regression\": MultiOutputClassifier(\n",
        "        LogisticRegression(max_iter=1000, random_state=42)\n",
        "    ),\n",
        "    \"Random Forest\": MultiOutputClassifier(\n",
        "        RandomForestClassifier(max_depth=10, min_samples_split=5, random_state=42)\n",
        "    )\n",
        "}\n",
        "\n",
        "def evaluate_model(model, embeddings, labels, dataset_name, model_name):\n",
        "    \"\"\"\n",
        "    Predict labels using the given model and embeddings, then print the classification report\n",
        "    and macro F1 score.\n",
        "    \"\"\"\n",
        "    predictions = model.predict(embeddings)\n",
        "    print(f\"\\nModel: {model_name} - {dataset_name}\")\n",
        "    print(\"Classification Report:\")\n",
        "    print(classification_report(labels, predictions, target_names=label_columns))\n",
        "\n",
        "    macro_f1 = f1_score(labels, predictions, average='macro')\n",
        "    print(f\"Macro F1 Score for {dataset_name}: {macro_f1}\\n\")\n",
        "    return macro_f1\n",
        "\n",
        "# Train and evaluate each model on both the training and testing sets.\n",
        "for model_name, clf in models.items():\n",
        "    print(\"==============================================\")\n",
        "    print(f\"Training {model_name}...\")\n",
        "    clf.fit(train_embeddings_scaled, train_labels)\n",
        "\n",
        "    # Evaluate on the Training Set\n",
        "    print(\"Evaluation on Train Set:\")\n",
        "    train_macro_f1 = evaluate_model(clf, train_embeddings_scaled, train_labels, \"Train Set\", model_name)\n",
        "\n",
        "    # Evaluate on the Testing Set\n",
        "    print(\"Evaluation on Test Set:\")\n",
        "    test_macro_f1 = evaluate_model(clf, test_embeddings_scaled, test_labels, \"Test Set\", model_name)\n",
        "\n",
        "    print(f\"Summary for {model_name}:\")\n",
        "    print(f\"  Train Macro F1 Score: {train_macro_f1}\")\n",
        "    print(f\"  Test Macro F1 Score:  {test_macro_f1}\")\n",
        "    print(\"==============================================\\n\")\n",
        "\n",
        "# -------------------------------\n",
        "# 4. Predict Emotions for New Text\n",
        "# -------------------------------\n",
        "\n",
        "def predict_emotions(text, model, scaler):\n",
        "    \"\"\"\n",
        "    Given a new text string, generate its embedding, scale it, and predict emotion labels.\n",
        "    Returns a dictionary mapping each emotion to its predicted label (0 or 1).\n",
        "    \"\"\"\n",
        "    # Generate embedding for the new text\n",
        "    embedding = generate_embeddings([text])\n",
        "    embedding_scaled = scaler.transform(embedding)\n",
        "\n",
        "    # Get predictions\n",
        "    predictions = model.predict(embedding_scaled)\n",
        "\n",
        "    # Map each label to its corresponding emotion\n",
        "    emotion_preds = {emotion: int(pred) for emotion, pred in zip(label_columns, predictions[0])}\n",
        "    return emotion_preds\n",
        "\n",
        "# Example usage for prediction using the SVM model (you can choose any of the trained models)\n",
        "example_text = \"यह एक उदाहरण वाक्य है\"\n",
        "print(\"Example prediction using SVM:\")\n",
        "svm_model = models[\"SVM\"]\n",
        "example_prediction = predict_emotions(example_text, svm_model, scaler)\n",
        "for emotion, pred in example_prediction.items():\n",
        "    print(f\"{emotion}: {pred}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f26n39GyUx_h",
        "outputId": "2a1f47f0-f1cd-4887-80f9-2a5b0de00993"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==============================================\n",
            "Training SVM...\n",
            "Evaluation on Train Set:\n",
            "\n",
            "Model: SVM - Train Set\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       anger       1.00      0.74      0.85       380\n",
            "     disgust       1.00      0.44      0.61        66\n",
            "        fear       1.00      0.67      0.80       119\n",
            "         joy       1.00      0.82      0.90       302\n",
            "     sadness       1.00      0.77      0.87       246\n",
            "    surprise       1.00      0.69      0.82       246\n",
            "\n",
            "   micro avg       1.00      0.74      0.85      1359\n",
            "   macro avg       1.00      0.69      0.81      1359\n",
            "weighted avg       1.00      0.74      0.84      1359\n",
            " samples avg       0.56      0.54      0.55      1359\n",
            "\n",
            "Macro F1 Score for Train Set: 0.8091595475180168\n",
            "\n",
            "Evaluation on Test Set:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model: SVM - Test Set\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       anger       0.89      0.25      0.40        63\n",
            "     disgust       0.00      0.00      0.00        11\n",
            "        fear       0.00      0.00      0.00        20\n",
            "         joy       0.83      0.30      0.44        50\n",
            "     sadness       0.85      0.41      0.56        41\n",
            "    surprise       1.00      0.05      0.10        39\n",
            "\n",
            "   micro avg       0.86      0.22      0.35       224\n",
            "   macro avg       0.60      0.17      0.25       224\n",
            "weighted avg       0.77      0.22      0.33       224\n",
            " samples avg       0.19      0.17      0.17       224\n",
            "\n",
            "Macro F1 Score for Test Set: 0.24852937062889682\n",
            "\n",
            "Summary for SVM:\n",
            "  Train Macro F1 Score: 0.8091595475180168\n",
            "  Test Macro F1 Score:  0.24852937062889682\n",
            "==============================================\n",
            "\n",
            "==============================================\n",
            "Training Naive Bayes...\n",
            "Evaluation on Train Set:\n",
            "\n",
            "Model: Naive Bayes - Train Set\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       anger       0.52      0.78      0.62       380\n",
            "     disgust       0.56      0.86      0.68        66\n",
            "        fear       0.28      0.91      0.43       119\n",
            "         joy       0.45      0.74      0.56       302\n",
            "     sadness       0.45      0.83      0.58       246\n",
            "    surprise       0.32      0.85      0.46       246\n",
            "\n",
            "   micro avg       0.41      0.81      0.54      1359\n",
            "   macro avg       0.43      0.83      0.55      1359\n",
            "weighted avg       0.43      0.81      0.56      1359\n",
            " samples avg       0.40      0.58      0.45      1359\n",
            "\n",
            "Macro F1 Score for Train Set: 0.5549254777496907\n",
            "\n",
            "Evaluation on Test Set:\n",
            "\n",
            "Model: Naive Bayes - Test Set\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       anger       0.49      0.67      0.56        63\n",
            "     disgust       0.31      0.45      0.37        11\n",
            "        fear       0.27      0.80      0.40        20\n",
            "         joy       0.37      0.62      0.46        50\n",
            "     sadness       0.39      0.78      0.52        41\n",
            "    surprise       0.24      0.74      0.37        39\n",
            "\n",
            "   micro avg       0.35      0.69      0.46       224\n",
            "   macro avg       0.35      0.68      0.45       224\n",
            "weighted avg       0.37      0.69      0.47       224\n",
            " samples avg       0.34      0.50      0.38       224\n",
            "\n",
            "Macro F1 Score for Test Set: 0.44737152294054394\n",
            "\n",
            "Summary for Naive Bayes:\n",
            "  Train Macro F1 Score: 0.5549254777496907\n",
            "  Test Macro F1 Score:  0.44737152294054394\n",
            "==============================================\n",
            "\n",
            "==============================================\n",
            "Training Logistic Regression...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation on Train Set:\n",
            "\n",
            "Model: Logistic Regression - Train Set\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       anger       1.00      1.00      1.00       380\n",
            "     disgust       1.00      1.00      1.00        66\n",
            "        fear       1.00      1.00      1.00       119\n",
            "         joy       1.00      1.00      1.00       302\n",
            "     sadness       1.00      1.00      1.00       246\n",
            "    surprise       1.00      1.00      1.00       246\n",
            "\n",
            "   micro avg       1.00      1.00      1.00      1359\n",
            "   macro avg       1.00      1.00      1.00      1359\n",
            "weighted avg       1.00      1.00      1.00      1359\n",
            " samples avg       0.72      0.72      0.72      1359\n",
            "\n",
            "Macro F1 Score for Train Set: 1.0\n",
            "\n",
            "Evaluation on Test Set:\n",
            "\n",
            "Model: Logistic Regression - Test Set\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       anger       0.50      0.43      0.46        63\n",
            "     disgust       0.25      0.18      0.21        11\n",
            "        fear       0.38      0.40      0.39        20\n",
            "         joy       0.51      0.52      0.51        50\n",
            "     sadness       0.52      0.61      0.56        41\n",
            "    surprise       0.41      0.38      0.39        39\n",
            "\n",
            "   micro avg       0.47      0.46      0.47       224\n",
            "   macro avg       0.43      0.42      0.42       224\n",
            "weighted avg       0.47      0.46      0.46       224\n",
            " samples avg       0.32      0.34      0.32       224\n",
            "\n",
            "Macro F1 Score for Test Set: 0.42228245997162106\n",
            "\n",
            "Summary for Logistic Regression:\n",
            "  Train Macro F1 Score: 1.0\n",
            "  Test Macro F1 Score:  0.42228245997162106\n",
            "==============================================\n",
            "\n",
            "==============================================\n",
            "Training Random Forest...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation on Train Set:\n",
            "\n",
            "Model: Random Forest - Train Set\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       anger       1.00      0.95      0.98       380\n",
            "     disgust       0.00      0.00      0.00        66\n",
            "        fear       1.00      0.42      0.59       119\n",
            "         joy       1.00      0.92      0.96       302\n",
            "     sadness       1.00      0.66      0.79       246\n",
            "    surprise       1.00      0.82      0.90       246\n",
            "\n",
            "   micro avg       1.00      0.77      0.87      1359\n",
            "   macro avg       0.83      0.63      0.70      1359\n",
            "weighted avg       0.95      0.77      0.84      1359\n",
            " samples avg       0.61      0.58      0.59      1359\n",
            "\n",
            "Macro F1 Score for Train Set: 0.7033637807748762\n",
            "\n",
            "Evaluation on Test Set:\n",
            "\n",
            "Model: Random Forest - Test Set\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       anger       1.00      0.02      0.03        63\n",
            "     disgust       0.00      0.00      0.00        11\n",
            "        fear       0.00      0.00      0.00        20\n",
            "         joy       1.00      0.06      0.11        50\n",
            "     sadness       1.00      0.05      0.09        41\n",
            "    surprise       0.00      0.00      0.00        39\n",
            "\n",
            "   micro avg       1.00      0.03      0.05       224\n",
            "   macro avg       0.50      0.02      0.04       224\n",
            "weighted avg       0.69      0.03      0.05       224\n",
            " samples avg       0.02      0.02      0.02       224\n",
            "\n",
            "Macro F1 Score for Test Set: 0.039580133830627466\n",
            "\n",
            "Summary for Random Forest:\n",
            "  Train Macro F1 Score: 0.7033637807748762\n",
            "  Test Macro F1 Score:  0.039580133830627466\n",
            "==============================================\n",
            "\n",
            "Example prediction using SVM:\n",
            "anger: 0\n",
            "disgust: 0\n",
            "fear: 0\n",
            "joy: 0\n",
            "sadness: 0\n",
            "surprise: 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# chinese"
      ],
      "metadata": {
        "id": "SU5aUPehVAoD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Import classifiers and wrappers\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.multioutput import MultiOutputClassifier\n",
        "\n",
        "# Evaluation metrics\n",
        "from sklearn.metrics import classification_report, f1_score\n",
        "\n",
        "# For generating text embeddings\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# For scaling the embeddings\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# -------------------------------\n",
        "# 1. Load Data and Generate Embeddings\n",
        "# -------------------------------\n",
        "\n",
        "# Load datasets\n",
        "train_df = pd.read_csv(\"/content/chn.csv\")\n",
        "test_df = pd.read_csv(\"/content/chn_dev.csv\")\n",
        "\n",
        "# Load the LaBSE model for generating embeddings\n",
        "labse_model = SentenceTransformer('intfloat/multilingual-e5-large')\n",
        "\n",
        "def generate_embeddings(texts):\n",
        "    \"\"\"\n",
        "    Given a list or array of texts, generate and return their normalized embeddings.\n",
        "    \"\"\"\n",
        "    return np.array(labse_model.encode(texts, convert_to_numpy=True, normalize_embeddings=True))\n",
        "\n",
        "# Get text data\n",
        "train_texts = train_df['text'].values\n",
        "test_texts = test_df['text'].values\n",
        "\n",
        "# Generate embeddings for train and test sets\n",
        "train_embeddings = generate_embeddings(train_texts)\n",
        "test_embeddings = generate_embeddings(test_texts)\n",
        "\n",
        "# -------------------------------\n",
        "# 2. Prepare Labels and Scale Features\n",
        "# -------------------------------\n",
        "\n",
        "# Define the emotion label columns\n",
        "label_columns = ['anger', 'disgust', 'fear', 'joy', 'sadness', 'surprise']\n",
        "\n",
        "# Extract label arrays\n",
        "train_labels = train_df[label_columns].values\n",
        "test_labels = test_df[label_columns].values\n",
        "\n",
        "# Scale the embeddings (feature standardization)\n",
        "scaler = StandardScaler()\n",
        "train_embeddings_scaled = scaler.fit_transform(train_embeddings)\n",
        "test_embeddings_scaled = scaler.transform(test_embeddings)\n",
        "\n",
        "# -------------------------------\n",
        "# 3. Define, Train, and Evaluate Models\n",
        "# -------------------------------\n",
        "\n",
        "# Define the four models. For Random Forest we add some regularization\n",
        "# by limiting the maximum tree depth and controlling the minimum samples needed for splits.\n",
        "models = {\n",
        "    \"SVM\": MultiOutputClassifier(SVC(kernel='rbf', probability=True, random_state=42)),\n",
        "    \"Naive Bayes\": MultiOutputClassifier(GaussianNB()),\n",
        "    \"Logistic Regression\": MultiOutputClassifier(\n",
        "        LogisticRegression(max_iter=1000, random_state=42)\n",
        "    ),\n",
        "    \"Random Forest\": MultiOutputClassifier(\n",
        "        RandomForestClassifier(max_depth=10, min_samples_split=5, random_state=42)\n",
        "    )\n",
        "}\n",
        "\n",
        "def evaluate_model(model, embeddings, labels, dataset_name, model_name):\n",
        "    \"\"\"\n",
        "    Predict labels using the given model and embeddings, then print the classification report\n",
        "    and macro F1 score.\n",
        "    \"\"\"\n",
        "    predictions = model.predict(embeddings)\n",
        "    print(f\"\\nModel: {model_name} - {dataset_name}\")\n",
        "    print(\"Classification Report:\")\n",
        "    print(classification_report(labels, predictions, target_names=label_columns))\n",
        "\n",
        "    macro_f1 = f1_score(labels, predictions, average='macro')\n",
        "    print(f\"Macro F1 Score for {dataset_name}: {macro_f1}\\n\")\n",
        "    return macro_f1\n",
        "\n",
        "# Train and evaluate each model on both the training and testing sets.\n",
        "for model_name, clf in models.items():\n",
        "    print(\"==============================================\")\n",
        "    print(f\"Training {model_name}...\")\n",
        "    clf.fit(train_embeddings_scaled, train_labels)\n",
        "\n",
        "    # Evaluate on the Training Set\n",
        "    print(\"Evaluation on Train Set:\")\n",
        "    train_macro_f1 = evaluate_model(clf, train_embeddings_scaled, train_labels, \"Train Set\", model_name)\n",
        "\n",
        "    # Evaluate on the Testing Set\n",
        "    print(\"Evaluation on Test Set:\")\n",
        "    test_macro_f1 = evaluate_model(clf, test_embeddings_scaled, test_labels, \"Test Set\", model_name)\n",
        "\n",
        "    print(f\"Summary for {model_name}:\")\n",
        "    print(f\"  Train Macro F1 Score: {train_macro_f1}\")\n",
        "    print(f\"  Test Macro F1 Score:  {test_macro_f1}\")\n",
        "    print(\"==============================================\\n\")\n",
        "\n",
        "# -------------------------------\n",
        "# 4. Predict Emotions for New Text\n",
        "# -------------------------------\n",
        "\n",
        "def predict_emotions(text, model, scaler):\n",
        "    \"\"\"\n",
        "    Given a new text string, generate its embedding, scale it, and predict emotion labels.\n",
        "    Returns a dictionary mapping each emotion to its predicted label (0 or 1).\n",
        "    \"\"\"\n",
        "    # Generate embedding for the new text\n",
        "    embedding = generate_embeddings([text])\n",
        "    embedding_scaled = scaler.transform(embedding)\n",
        "\n",
        "    # Get predictions\n",
        "    predictions = model.predict(embedding_scaled)\n",
        "\n",
        "    # Map each label to its corresponding emotion\n",
        "    emotion_preds = {emotion: int(pred) for emotion, pred in zip(label_columns, predictions[0])}\n",
        "    return emotion_preds\n",
        "\n",
        "# Example usage for prediction using the SVM model (you can choose any of the trained models)\n",
        "example_text = \"यह एक उदाहरण वाक्य है\"\n",
        "print(\"Example prediction using SVM:\")\n",
        "svm_model = models[\"SVM\"]\n",
        "example_prediction = predict_emotions(example_text, svm_model, scaler)\n",
        "for emotion, pred in example_prediction.items():\n",
        "    print(f\"{emotion}: {pred}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B2_xRHBfVDlL",
        "outputId": "5337f0a5-a610-4ff7-a80b-3aef45b23616"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==============================================\n",
            "Training SVM...\n",
            "Evaluation on Train Set:\n",
            "\n",
            "Model: SVM - Train Set\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       anger       0.96      0.98      0.97      1178\n",
            "     disgust       1.00      0.42      0.59       403\n",
            "        fear       1.00      0.23      0.37        71\n",
            "         joy       0.99      0.96      0.98       529\n",
            "     sadness       1.00      0.66      0.79       354\n",
            "    surprise       1.00      0.34      0.51       178\n",
            "\n",
            "   micro avg       0.98      0.79      0.87      2713\n",
            "   macro avg       0.99      0.60      0.70      2713\n",
            "weighted avg       0.98      0.79      0.85      2713\n",
            " samples avg       0.70      0.64      0.66      2713\n",
            "\n",
            "Macro F1 Score for Train Set: 0.7020999868455745\n",
            "\n",
            "Evaluation on Test Set:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model: SVM - Test Set\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       anger       0.78      0.89      0.83        92\n",
            "     disgust       0.00      0.00      0.00        32\n",
            "        fear       0.00      0.00      0.00         5\n",
            "         joy       0.94      0.84      0.89        37\n",
            "     sadness       0.71      0.23      0.34        22\n",
            "    surprise       0.00      0.00      0.00        17\n",
            "\n",
            "   micro avg       0.81      0.58      0.67       205\n",
            "   macro avg       0.41      0.33      0.34       205\n",
            "weighted avg       0.60      0.58      0.57       205\n",
            " samples avg       0.58      0.48      0.51       205\n",
            "\n",
            "Macro F1 Score for Test Set: 0.34383819692764206\n",
            "\n",
            "Summary for SVM:\n",
            "  Train Macro F1 Score: 0.7020999868455745\n",
            "  Test Macro F1 Score:  0.34383819692764206\n",
            "==============================================\n",
            "\n",
            "==============================================\n",
            "Training Naive Bayes...\n",
            "Evaluation on Train Set:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model: Naive Bayes - Train Set\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       anger       0.72      0.84      0.77      1178\n",
            "     disgust       0.30      0.79      0.44       403\n",
            "        fear       0.15      0.86      0.25        71\n",
            "         joy       0.75      0.85      0.80       529\n",
            "     sadness       0.40      0.79      0.53       354\n",
            "    surprise       0.25      0.88      0.39       178\n",
            "\n",
            "   micro avg       0.47      0.83      0.60      2713\n",
            "   macro avg       0.43      0.83      0.53      2713\n",
            "weighted avg       0.58      0.83      0.66      2713\n",
            " samples avg       0.45      0.64      0.50      2713\n",
            "\n",
            "Macro F1 Score for Train Set: 0.5300024744167556\n",
            "\n",
            "Evaluation on Test Set:\n",
            "\n",
            "Model: Naive Bayes - Test Set\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       anger       0.78      0.88      0.83        92\n",
            "     disgust       0.28      0.78      0.41        32\n",
            "        fear       0.14      0.80      0.24         5\n",
            "         joy       0.77      0.92      0.84        37\n",
            "     sadness       0.29      0.59      0.39        22\n",
            "    surprise       0.25      0.76      0.38        17\n",
            "\n",
            "   micro avg       0.47      0.83      0.60       205\n",
            "   macro avg       0.42      0.79      0.52       205\n",
            "weighted avg       0.59      0.83      0.67       205\n",
            " samples avg       0.45      0.65      0.51       205\n",
            "\n",
            "Macro F1 Score for Test Set: 0.515349468445587\n",
            "\n",
            "Summary for Naive Bayes:\n",
            "  Train Macro F1 Score: 0.5300024744167556\n",
            "  Test Macro F1 Score:  0.515349468445587\n",
            "==============================================\n",
            "\n",
            "==============================================\n",
            "Training Logistic Regression...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation on Train Set:\n",
            "\n",
            "Model: Logistic Regression - Train Set\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       anger       0.99      0.99      0.99      1178\n",
            "     disgust       1.00      0.97      0.98       403\n",
            "        fear       1.00      1.00      1.00        71\n",
            "         joy       1.00      1.00      1.00       529\n",
            "     sadness       1.00      1.00      1.00       354\n",
            "    surprise       1.00      1.00      1.00       178\n",
            "\n",
            "   micro avg       1.00      0.99      1.00      2713\n",
            "   macro avg       1.00      0.99      1.00      2713\n",
            "weighted avg       1.00      0.99      1.00      2713\n",
            " samples avg       0.77      0.77      0.77      2713\n",
            "\n",
            "Macro F1 Score for Train Set: 0.9964907291386017\n",
            "\n",
            "Evaluation on Test Set:\n",
            "\n",
            "Model: Logistic Regression - Test Set\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       anger       0.69      0.71      0.70        92\n",
            "     disgust       0.36      0.44      0.39        32\n",
            "        fear       0.60      0.60      0.60         5\n",
            "         joy       0.94      0.78      0.85        37\n",
            "     sadness       0.25      0.32      0.28        22\n",
            "    surprise       0.62      0.59      0.61        17\n",
            "\n",
            "   micro avg       0.60      0.62      0.61       205\n",
            "   macro avg       0.58      0.57      0.57       205\n",
            "weighted avg       0.63      0.62      0.62       205\n",
            " samples avg       0.46      0.48      0.45       205\n",
            "\n",
            "Macro F1 Score for Test Set: 0.5720487851495147\n",
            "\n",
            "Summary for Logistic Regression:\n",
            "  Train Macro F1 Score: 0.9964907291386017\n",
            "  Test Macro F1 Score:  0.5720487851495147\n",
            "==============================================\n",
            "\n",
            "==============================================\n",
            "Training Random Forest...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation on Train Set:\n",
            "\n",
            "Model: Random Forest - Train Set\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       anger       0.99      1.00      1.00      1178\n",
            "     disgust       1.00      0.44      0.61       403\n",
            "        fear       0.00      0.00      0.00        71\n",
            "         joy       1.00      0.95      0.97       529\n",
            "     sadness       1.00      0.50      0.66       354\n",
            "    surprise       0.00      0.00      0.00       178\n",
            "\n",
            "   micro avg       1.00      0.75      0.86      2713\n",
            "   macro avg       0.67      0.48      0.54      2713\n",
            "weighted avg       0.91      0.75      0.80      2713\n",
            " samples avg       0.68      0.60      0.63      2713\n",
            "\n",
            "Macro F1 Score for Train Set: 0.5409566762115132\n",
            "\n",
            "Evaluation on Test Set:\n",
            "\n",
            "Model: Random Forest - Test Set\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       anger       0.80      0.85      0.83        92\n",
            "     disgust       0.00      0.00      0.00        32\n",
            "        fear       0.00      0.00      0.00         5\n",
            "         joy       0.95      0.51      0.67        37\n",
            "     sadness       0.00      0.00      0.00        22\n",
            "    surprise       0.00      0.00      0.00        17\n",
            "\n",
            "   micro avg       0.83      0.47      0.60       205\n",
            "   macro avg       0.29      0.23      0.25       205\n",
            "weighted avg       0.53      0.47      0.49       205\n",
            " samples avg       0.48      0.39      0.42       205\n",
            "\n",
            "Macro F1 Score for Test Set: 0.2486772486772487\n",
            "\n",
            "Summary for Random Forest:\n",
            "  Train Macro F1 Score: 0.5409566762115132\n",
            "  Test Macro F1 Score:  0.2486772486772487\n",
            "==============================================\n",
            "\n",
            "Example prediction using SVM:\n",
            "anger: 0\n",
            "disgust: 0\n",
            "fear: 0\n",
            "joy: 0\n",
            "sadness: 0\n",
            "surprise: 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# german"
      ],
      "metadata": {
        "id": "SF6h6HcNVdza"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Import classifiers and wrappers\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.multioutput import MultiOutputClassifier\n",
        "\n",
        "# Evaluation metrics\n",
        "from sklearn.metrics import classification_report, f1_score\n",
        "\n",
        "# For generating text embeddings\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# For scaling the embeddings\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# -------------------------------\n",
        "# 1. Load Data and Generate Embeddings\n",
        "# -------------------------------\n",
        "\n",
        "# Load datasets\n",
        "train_df = pd.read_csv(\"/content/deu.csv\")\n",
        "test_df = pd.read_csv(\"/content/deu_dev.csv\")\n",
        "\n",
        "# Load the LaBSE model for generating embeddings\n",
        "labse_model = SentenceTransformer('intfloat/multilingual-e5-large')\n",
        "\n",
        "def generate_embeddings(texts):\n",
        "    \"\"\"\n",
        "    Given a list or array of texts, generate and return their normalized embeddings.\n",
        "    \"\"\"\n",
        "    return np.array(labse_model.encode(texts, convert_to_numpy=True, normalize_embeddings=True))\n",
        "\n",
        "# Get text data\n",
        "train_texts = train_df['text'].values\n",
        "test_texts = test_df['text'].values\n",
        "\n",
        "# Generate embeddings for train and test sets\n",
        "train_embeddings = generate_embeddings(train_texts)\n",
        "test_embeddings = generate_embeddings(test_texts)\n",
        "\n",
        "# -------------------------------\n",
        "# 2. Prepare Labels and Scale Features\n",
        "# -------------------------------\n",
        "\n",
        "# Define the emotion label columns\n",
        "label_columns = ['anger', 'disgust', 'fear', 'joy', 'sadness', 'surprise']\n",
        "\n",
        "# Extract label arrays\n",
        "train_labels = train_df[label_columns].values\n",
        "test_labels = test_df[label_columns].values\n",
        "\n",
        "# Scale the embeddings (feature standardization)\n",
        "scaler = StandardScaler()\n",
        "train_embeddings_scaled = scaler.fit_transform(train_embeddings)\n",
        "test_embeddings_scaled = scaler.transform(test_embeddings)\n",
        "\n",
        "# -------------------------------\n",
        "# 3. Define, Train, and Evaluate Models\n",
        "# -------------------------------\n",
        "\n",
        "# Define the four models. For Random Forest we add some regularization\n",
        "# by limiting the maximum tree depth and controlling the minimum samples needed for splits.\n",
        "models = {\n",
        "    \"SVM\": MultiOutputClassifier(SVC(kernel='rbf', probability=True, random_state=42)),\n",
        "    \"Naive Bayes\": MultiOutputClassifier(GaussianNB()),\n",
        "    \"Logistic Regression\": MultiOutputClassifier(\n",
        "        LogisticRegression(max_iter=1000, random_state=42)\n",
        "    ),\n",
        "    \"Random Forest\": MultiOutputClassifier(\n",
        "        RandomForestClassifier(max_depth=10, min_samples_split=5, random_state=42)\n",
        "    )\n",
        "}\n",
        "\n",
        "def evaluate_model(model, embeddings, labels, dataset_name, model_name):\n",
        "    \"\"\"\n",
        "    Predict labels using the given model and embeddings, then print the classification report\n",
        "    and macro F1 score.\n",
        "    \"\"\"\n",
        "    predictions = model.predict(embeddings)\n",
        "    print(f\"\\nModel: {model_name} - {dataset_name}\")\n",
        "    print(\"Classification Report:\")\n",
        "    print(classification_report(labels, predictions, target_names=label_columns))\n",
        "\n",
        "    macro_f1 = f1_score(labels, predictions, average='macro')\n",
        "    print(f\"Macro F1 Score for {dataset_name}: {macro_f1}\\n\")\n",
        "    return macro_f1\n",
        "\n",
        "# Train and evaluate each model on both the training and testing sets.\n",
        "for model_name, clf in models.items():\n",
        "    print(\"==============================================\")\n",
        "    print(f\"Training {model_name}...\")\n",
        "    clf.fit(train_embeddings_scaled, train_labels)\n",
        "\n",
        "    # Evaluate on the Training Set\n",
        "    print(\"Evaluation on Train Set:\")\n",
        "    train_macro_f1 = evaluate_model(clf, train_embeddings_scaled, train_labels, \"Train Set\", model_name)\n",
        "\n",
        "    # Evaluate on the Testing Set\n",
        "    print(\"Evaluation on Test Set:\")\n",
        "    test_macro_f1 = evaluate_model(clf, test_embeddings_scaled, test_labels, \"Test Set\", model_name)\n",
        "\n",
        "    print(f\"Summary for {model_name}:\")\n",
        "    print(f\"  Train Macro F1 Score: {train_macro_f1}\")\n",
        "    print(f\"  Test Macro F1 Score:  {test_macro_f1}\")\n",
        "    print(\"==============================================\\n\")\n",
        "\n",
        "# -------------------------------\n",
        "# 4. Predict Emotions for New Text\n",
        "# -------------------------------\n",
        "\n",
        "def predict_emotions(text, model, scaler):\n",
        "    \"\"\"\n",
        "    Given a new text string, generate its embedding, scale it, and predict emotion labels.\n",
        "    Returns a dictionary mapping each emotion to its predicted label (0 or 1).\n",
        "    \"\"\"\n",
        "    # Generate embedding for the new text\n",
        "    embedding = generate_embeddings([text])\n",
        "    embedding_scaled = scaler.transform(embedding)\n",
        "\n",
        "    # Get predictions\n",
        "    predictions = model.predict(embedding_scaled)\n",
        "\n",
        "    # Map each label to its corresponding emotion\n",
        "    emotion_preds = {emotion: int(pred) for emotion, pred in zip(label_columns, predictions[0])}\n",
        "    return emotion_preds\n",
        "\n",
        "# Example usage for prediction using the SVM model (you can choose any of the trained models)\n",
        "example_text = \"यह एक उदाहरण वाक्य है\"\n",
        "print(\"Example prediction using SVM:\")\n",
        "svm_model = models[\"SVM\"]\n",
        "example_prediction = predict_emotions(example_text, svm_model, scaler)\n",
        "for emotion, pred in example_prediction.items():\n",
        "    print(f\"{emotion}: {pred}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uU86bOMdVjfv",
        "outputId": "8ab2d9f6-a16c-4182-cb69-83fd42e7c728"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==============================================\n",
            "Training SVM...\n",
            "Evaluation on Train Set:\n",
            "\n",
            "Model: SVM - Train Set\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       anger       0.97      0.93      0.95       768\n",
            "     disgust       0.96      0.92      0.94       832\n",
            "        fear       1.00      0.48      0.65       239\n",
            "         joy       0.99      0.86      0.92       541\n",
            "     sadness       1.00      0.80      0.89       516\n",
            "    surprise       1.00      0.39      0.56       159\n",
            "\n",
            "   micro avg       0.98      0.83      0.90      3055\n",
            "   macro avg       0.99      0.73      0.82      3055\n",
            "weighted avg       0.98      0.83      0.89      3055\n",
            " samples avg       0.65      0.62      0.63      3055\n",
            "\n",
            "Macro F1 Score for Train Set: 0.8177280794444788\n",
            "\n",
            "Evaluation on Test Set:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model: SVM - Test Set\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       anger       0.74      0.66      0.70        61\n",
            "     disgust       0.71      0.59      0.64        61\n",
            "        fear       0.00      0.00      0.00        20\n",
            "         joy       0.95      0.47      0.63        40\n",
            "     sadness       0.94      0.31      0.47        48\n",
            "    surprise       0.00      0.00      0.00        15\n",
            "\n",
            "   micro avg       0.78      0.45      0.57       245\n",
            "   macro avg       0.56      0.34      0.41       245\n",
            "weighted avg       0.70      0.45      0.53       245\n",
            " samples avg       0.37      0.34      0.34       245\n",
            "\n",
            "Macro F1 Score for Test Set: 0.4067654416839199\n",
            "\n",
            "Summary for SVM:\n",
            "  Train Macro F1 Score: 0.8177280794444788\n",
            "  Test Macro F1 Score:  0.4067654416839199\n",
            "==============================================\n",
            "\n",
            "==============================================\n",
            "Training Naive Bayes...\n",
            "Evaluation on Train Set:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model: Naive Bayes - Train Set\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       anger       0.72      0.80      0.76       768\n",
            "     disgust       0.68      0.74      0.71       832\n",
            "        fear       0.32      0.81      0.46       239\n",
            "         joy       0.55      0.80      0.65       541\n",
            "     sadness       0.52      0.75      0.61       516\n",
            "    surprise       0.41      0.85      0.55       159\n",
            "\n",
            "   micro avg       0.56      0.78      0.65      3055\n",
            "   macro avg       0.53      0.79      0.62      3055\n",
            "weighted avg       0.60      0.78      0.67      3055\n",
            " samples avg       0.48      0.58      0.51      3055\n",
            "\n",
            "Macro F1 Score for Train Set: 0.6236856738458265\n",
            "\n",
            "Evaluation on Test Set:\n",
            "\n",
            "Model: Naive Bayes - Test Set\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       anger       0.68      0.74      0.71        61\n",
            "     disgust       0.59      0.67      0.63        61\n",
            "        fear       0.18      0.45      0.26        20\n",
            "         joy       0.51      0.68      0.58        40\n",
            "     sadness       0.57      0.69      0.62        48\n",
            "    surprise       0.19      0.27      0.22        15\n",
            "\n",
            "   micro avg       0.50      0.65      0.57       245\n",
            "   macro avg       0.45      0.58      0.50       245\n",
            "weighted avg       0.54      0.65      0.58       245\n",
            " samples avg       0.41      0.49      0.42       245\n",
            "\n",
            "Macro F1 Score for Test Set: 0.5036803996969049\n",
            "\n",
            "Summary for Naive Bayes:\n",
            "  Train Macro F1 Score: 0.6236856738458265\n",
            "  Test Macro F1 Score:  0.5036803996969049\n",
            "==============================================\n",
            "\n",
            "==============================================\n",
            "Training Logistic Regression...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation on Train Set:\n",
            "\n",
            "Model: Logistic Regression - Train Set\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       anger       1.00      1.00      1.00       768\n",
            "     disgust       1.00      0.99      0.99       832\n",
            "        fear       1.00      1.00      1.00       239\n",
            "         joy       1.00      1.00      1.00       541\n",
            "     sadness       1.00      1.00      1.00       516\n",
            "    surprise       1.00      1.00      1.00       159\n",
            "\n",
            "   micro avg       1.00      1.00      1.00      3055\n",
            "   macro avg       1.00      1.00      1.00      3055\n",
            "weighted avg       1.00      1.00      1.00      3055\n",
            " samples avg       0.75      0.75      0.75      3055\n",
            "\n",
            "Macro F1 Score for Train Set: 0.9986261555419622\n",
            "\n",
            "Evaluation on Test Set:\n",
            "\n",
            "Model: Logistic Regression - Test Set\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       anger       0.70      0.61      0.65        61\n",
            "     disgust       0.58      0.66      0.62        61\n",
            "        fear       0.33      0.35      0.34        20\n",
            "         joy       0.59      0.57      0.58        40\n",
            "     sadness       0.65      0.65      0.65        48\n",
            "    surprise       0.25      0.13      0.17        15\n",
            "\n",
            "   micro avg       0.59      0.57      0.58       245\n",
            "   macro avg       0.52      0.49      0.50       245\n",
            "weighted avg       0.58      0.57      0.57       245\n",
            " samples avg       0.39      0.41      0.38       245\n",
            "\n",
            "Macro F1 Score for Test Set: 0.501332615810093\n",
            "\n",
            "Summary for Logistic Regression:\n",
            "  Train Macro F1 Score: 0.9986261555419622\n",
            "  Test Macro F1 Score:  0.501332615810093\n",
            "==============================================\n",
            "\n",
            "==============================================\n",
            "Training Random Forest...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation on Train Set:\n",
            "\n",
            "Model: Random Forest - Train Set\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       anger       1.00      0.97      0.98       768\n",
            "     disgust       1.00      0.96      0.98       832\n",
            "        fear       1.00      0.05      0.09       239\n",
            "         joy       1.00      0.86      0.93       541\n",
            "     sadness       1.00      0.77      0.87       516\n",
            "    surprise       0.00      0.00      0.00       159\n",
            "\n",
            "   micro avg       1.00      0.79      0.88      3055\n",
            "   macro avg       0.83      0.60      0.64      3055\n",
            "weighted avg       0.95      0.79      0.83      3055\n",
            " samples avg       0.64      0.59      0.61      3055\n",
            "\n",
            "Macro F1 Score for Train Set: 0.6412383263418913\n",
            "\n",
            "Evaluation on Test Set:\n",
            "\n",
            "Model: Random Forest - Test Set\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       anger       0.78      0.41      0.54        61\n",
            "     disgust       0.75      0.39      0.52        61\n",
            "        fear       0.00      0.00      0.00        20\n",
            "         joy       1.00      0.05      0.10        40\n",
            "     sadness       1.00      0.02      0.04        48\n",
            "    surprise       0.00      0.00      0.00        15\n",
            "\n",
            "   micro avg       0.78      0.21      0.33       245\n",
            "   macro avg       0.59      0.15      0.20       245\n",
            "weighted avg       0.74      0.21      0.29       245\n",
            " samples avg       0.17      0.13      0.14       245\n",
            "\n",
            "Macro F1 Score for Test Set: 0.19830297710482045\n",
            "\n",
            "Summary for Random Forest:\n",
            "  Train Macro F1 Score: 0.6412383263418913\n",
            "  Test Macro F1 Score:  0.19830297710482045\n",
            "==============================================\n",
            "\n",
            "Example prediction using SVM:\n",
            "anger: 0\n",
            "disgust: 0\n",
            "fear: 0\n",
            "joy: 0\n",
            "sadness: 0\n",
            "surprise: 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# hausa"
      ],
      "metadata": {
        "id": "DPJE5uZsV5L4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Import classifiers and wrappers\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.multioutput import MultiOutputClassifier\n",
        "\n",
        "# Evaluation metrics\n",
        "from sklearn.metrics import classification_report, f1_score\n",
        "\n",
        "# For generating text embeddings\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# For scaling the embeddings\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# -------------------------------\n",
        "# 1. Load Data and Generate Embeddings\n",
        "# -------------------------------\n",
        "\n",
        "# Load datasets\n",
        "train_df = pd.read_csv(\"/content/hau.csv\")\n",
        "test_df = pd.read_csv(\"/content/hau_dev.csv\")\n",
        "\n",
        "# Load the LaBSE model for generating embeddings\n",
        "labse_model = SentenceTransformer('intfloat/multilingual-e5-large')\n",
        "\n",
        "def generate_embeddings(texts):\n",
        "    \"\"\"\n",
        "    Given a list or array of texts, generate and return their normalized embeddings.\n",
        "    \"\"\"\n",
        "    return np.array(labse_model.encode(texts, convert_to_numpy=True, normalize_embeddings=True))\n",
        "\n",
        "# Get text data\n",
        "train_texts = train_df['text'].values\n",
        "test_texts = test_df['text'].values\n",
        "\n",
        "# Generate embeddings for train and test sets\n",
        "train_embeddings = generate_embeddings(train_texts)\n",
        "test_embeddings = generate_embeddings(test_texts)\n",
        "\n",
        "# -------------------------------\n",
        "# 2. Prepare Labels and Scale Features\n",
        "# -------------------------------\n",
        "\n",
        "# Define the emotion label columns\n",
        "label_columns = ['anger', 'disgust', 'fear', 'joy', 'sadness', 'surprise']\n",
        "\n",
        "# Extract label arrays\n",
        "train_labels = train_df[label_columns].values\n",
        "test_labels = test_df[label_columns].values\n",
        "\n",
        "# Scale the embeddings (feature standardization)\n",
        "scaler = StandardScaler()\n",
        "train_embeddings_scaled = scaler.fit_transform(train_embeddings)\n",
        "test_embeddings_scaled = scaler.transform(test_embeddings)\n",
        "\n",
        "# -------------------------------\n",
        "# 3. Define, Train, and Evaluate Models\n",
        "# -------------------------------\n",
        "\n",
        "# Define the four models. For Random Forest we add some regularization\n",
        "# by limiting the maximum tree depth and controlling the minimum samples needed for splits.\n",
        "models = {\n",
        "    \"SVM\": MultiOutputClassifier(SVC(kernel='rbf', probability=True, random_state=42)),\n",
        "    \"Naive Bayes\": MultiOutputClassifier(GaussianNB()),\n",
        "    \"Logistic Regression\": MultiOutputClassifier(\n",
        "        LogisticRegression(max_iter=1000, random_state=42)\n",
        "    ),\n",
        "    \"Random Forest\": MultiOutputClassifier(\n",
        "        RandomForestClassifier(max_depth=10, min_samples_split=5, random_state=42)\n",
        "    )\n",
        "}\n",
        "\n",
        "def evaluate_model(model, embeddings, labels, dataset_name, model_name):\n",
        "    \"\"\"\n",
        "    Predict labels using the given model and embeddings, then print the classification report\n",
        "    and macro F1 score.\n",
        "    \"\"\"\n",
        "    predictions = model.predict(embeddings)\n",
        "    print(f\"\\nModel: {model_name} - {dataset_name}\")\n",
        "    print(\"Classification Report:\")\n",
        "    print(classification_report(labels, predictions, target_names=label_columns))\n",
        "\n",
        "    macro_f1 = f1_score(labels, predictions, average='macro')\n",
        "    print(f\"Macro F1 Score for {dataset_name}: {macro_f1}\\n\")\n",
        "    return macro_f1\n",
        "\n",
        "# Train and evaluate each model on both the training and testing sets.\n",
        "for model_name, clf in models.items():\n",
        "    print(\"==============================================\")\n",
        "    print(f\"Training {model_name}...\")\n",
        "    clf.fit(train_embeddings_scaled, train_labels)\n",
        "\n",
        "    # Evaluate on the Training Set\n",
        "    print(\"Evaluation on Train Set:\")\n",
        "    train_macro_f1 = evaluate_model(clf, train_embeddings_scaled, train_labels, \"Train Set\", model_name)\n",
        "\n",
        "    # Evaluate on the Testing Set\n",
        "    print(\"Evaluation on Test Set:\")\n",
        "    test_macro_f1 = evaluate_model(clf, test_embeddings_scaled, test_labels, \"Test Set\", model_name)\n",
        "\n",
        "    print(f\"Summary for {model_name}:\")\n",
        "    print(f\"  Train Macro F1 Score: {train_macro_f1}\")\n",
        "    print(f\"  Test Macro F1 Score:  {test_macro_f1}\")\n",
        "    print(\"==============================================\\n\")\n",
        "\n",
        "# -------------------------------\n",
        "# 4. Predict Emotions for New Text\n",
        "# -------------------------------\n",
        "\n",
        "def predict_emotions(text, model, scaler):\n",
        "    \"\"\"\n",
        "    Given a new text string, generate its embedding, scale it, and predict emotion labels.\n",
        "    Returns a dictionary mapping each emotion to its predicted label (0 or 1).\n",
        "    \"\"\"\n",
        "    # Generate embedding for the new text\n",
        "    embedding = generate_embeddings([text])\n",
        "    embedding_scaled = scaler.transform(embedding)\n",
        "\n",
        "    # Get predictions\n",
        "    predictions = model.predict(embedding_scaled)\n",
        "\n",
        "    # Map each label to its corresponding emotion\n",
        "    emotion_preds = {emotion: int(pred) for emotion, pred in zip(label_columns, predictions[0])}\n",
        "    return emotion_preds\n",
        "\n",
        "# Example usage for prediction using the SVM model (you can choose any of the trained models)\n",
        "example_text = \"यह एक उदाहरण वाक्य है\"\n",
        "print(\"Example prediction using SVM:\")\n",
        "svm_model = models[\"SVM\"]\n",
        "example_prediction = predict_emotions(example_text, svm_model, scaler)\n",
        "for emotion, pred in example_prediction.items():\n",
        "    print(f\"{emotion}: {pred}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hcQdMv1xV6ve",
        "outputId": "504019e4-2ad6-4018-bc18-15eb22179ebd"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==============================================\n",
            "Training SVM...\n",
            "Evaluation on Train Set:\n",
            "\n",
            "Model: SVM - Train Set\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       anger       1.00      0.74      0.85       408\n",
            "     disgust       1.00      0.75      0.86       329\n",
            "        fear       0.99      0.89      0.94       327\n",
            "         joy       0.99      0.76      0.86       320\n",
            "     sadness       0.96      0.88      0.92       647\n",
            "    surprise       1.00      0.75      0.86       349\n",
            "\n",
            "   micro avg       0.99      0.81      0.89      2380\n",
            "   macro avg       0.99      0.80      0.88      2380\n",
            "weighted avg       0.99      0.81      0.88      2380\n",
            " samples avg       0.74      0.71      0.72      2380\n",
            "\n",
            "Macro F1 Score for Train Set: 0.880511802269987\n",
            "\n",
            "Evaluation on Test Set:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model: SVM - Test Set\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       anger       0.70      0.21      0.32        67\n",
            "     disgust       0.86      0.56      0.68        55\n",
            "        fear       0.88      0.53      0.66        53\n",
            "         joy       0.89      0.45      0.60        53\n",
            "     sadness       0.82      0.51      0.63       109\n",
            "    surprise       0.95      0.32      0.47        57\n",
            "\n",
            "   micro avg       0.85      0.43      0.57       394\n",
            "   macro avg       0.85      0.43      0.56       394\n",
            "weighted avg       0.84      0.43      0.56       394\n",
            " samples avg       0.42      0.40      0.40       394\n",
            "\n",
            "Macro F1 Score for Test Set: 0.5614056438830756\n",
            "\n",
            "Summary for SVM:\n",
            "  Train Macro F1 Score: 0.880511802269987\n",
            "  Test Macro F1 Score:  0.5614056438830756\n",
            "==============================================\n",
            "\n",
            "==============================================\n",
            "Training Naive Bayes...\n",
            "Evaluation on Train Set:\n",
            "\n",
            "Model: Naive Bayes - Train Set\n",
            "Classification Report:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "       anger       0.44      0.73      0.54       408\n",
            "     disgust       0.53      0.76      0.62       329\n",
            "        fear       0.49      0.73      0.59       327\n",
            "         joy       0.51      0.74      0.60       320\n",
            "     sadness       0.58      0.76      0.66       647\n",
            "    surprise       0.39      0.69      0.50       349\n",
            "\n",
            "   micro avg       0.49      0.74      0.59      2380\n",
            "   macro avg       0.49      0.74      0.59      2380\n",
            "weighted avg       0.50      0.74      0.59      2380\n",
            " samples avg       0.50      0.65      0.54      2380\n",
            "\n",
            "Macro F1 Score for Train Set: 0.5860181093750018\n",
            "\n",
            "Evaluation on Test Set:\n",
            "\n",
            "Model: Naive Bayes - Test Set\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       anger       0.42      0.66      0.51        67\n",
            "     disgust       0.50      0.76      0.60        55\n",
            "        fear       0.51      0.74      0.60        53\n",
            "         joy       0.41      0.60      0.48        53\n",
            "     sadness       0.59      0.69      0.64       109\n",
            "    surprise       0.38      0.65      0.48        57\n",
            "\n",
            "   micro avg       0.47      0.68      0.56       394\n",
            "   macro avg       0.47      0.68      0.55       394\n",
            "weighted avg       0.48      0.68      0.56       394\n",
            " samples avg       0.47      0.61      0.51       394\n",
            "\n",
            "Macro F1 Score for Test Set: 0.55359280037283\n",
            "\n",
            "Summary for Naive Bayes:\n",
            "  Train Macro F1 Score: 0.5860181093750018\n",
            "  Test Macro F1 Score:  0.55359280037283\n",
            "==============================================\n",
            "\n",
            "==============================================\n",
            "Training Logistic Regression...\n",
            "Evaluation on Train Set:\n",
            "\n",
            "Model: Logistic Regression - Train Set\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       anger       1.00      1.00      1.00       408\n",
            "     disgust       1.00      1.00      1.00       329\n",
            "        fear       1.00      1.00      1.00       327\n",
            "         joy       1.00      1.00      1.00       320\n",
            "     sadness       0.99      0.99      0.99       647\n",
            "    surprise       1.00      1.00      1.00       349\n",
            "\n",
            "   micro avg       1.00      1.00      1.00      2380\n",
            "   macro avg       1.00      1.00      1.00      2380\n",
            "weighted avg       1.00      1.00      1.00      2380\n",
            " samples avg       0.87      0.87      0.87      2380\n",
            "\n",
            "Macro F1 Score for Train Set: 0.9980724749421742\n",
            "\n",
            "Evaluation on Test Set:\n",
            "\n",
            "Model: Logistic Regression - Test Set\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       anger       0.48      0.48      0.48        67\n",
            "     disgust       0.54      0.55      0.54        55\n",
            "        fear       0.70      0.62      0.66        53\n",
            "         joy       0.55      0.60      0.58        53\n",
            "     sadness       0.56      0.56      0.56       109\n",
            "    surprise       0.41      0.46      0.43        57\n",
            "\n",
            "   micro avg       0.54      0.54      0.54       394\n",
            "   macro avg       0.54      0.54      0.54       394\n",
            "weighted avg       0.54      0.54      0.54       394\n",
            " samples avg       0.45      0.49      0.45       394\n",
            "\n",
            "Macro F1 Score for Test Set: 0.541712395385963\n",
            "\n",
            "Summary for Logistic Regression:\n",
            "  Train Macro F1 Score: 0.9980724749421742\n",
            "  Test Macro F1 Score:  0.541712395385963\n",
            "==============================================\n",
            "\n",
            "==============================================\n",
            "Training Random Forest...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation on Train Set:\n",
            "\n",
            "Model: Random Forest - Train Set\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       anger       1.00      0.75      0.86       408\n",
            "     disgust       1.00      0.76      0.87       329\n",
            "        fear       1.00      0.82      0.90       327\n",
            "         joy       1.00      0.71      0.83       320\n",
            "     sadness       1.00      0.95      0.97       647\n",
            "    surprise       1.00      0.59      0.74       349\n",
            "\n",
            "   micro avg       1.00      0.79      0.88      2380\n",
            "   macro avg       1.00      0.76      0.86      2380\n",
            "weighted avg       1.00      0.79      0.88      2380\n",
            " samples avg       0.72      0.69      0.70      2380\n",
            "\n",
            "Macro F1 Score for Train Set: 0.8616289787433558\n",
            "\n",
            "Evaluation on Test Set:\n",
            "\n",
            "Model: Random Forest - Test Set\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       anger       0.00      0.00      0.00        67\n",
            "     disgust       1.00      0.24      0.38        55\n",
            "        fear       0.88      0.28      0.43        53\n",
            "         joy       1.00      0.09      0.17        53\n",
            "     sadness       0.86      0.33      0.48       109\n",
            "    surprise       1.00      0.04      0.07        57\n",
            "\n",
            "   micro avg       0.90      0.18      0.30       394\n",
            "   macro avg       0.79      0.16      0.25       394\n",
            "weighted avg       0.77      0.18      0.28       394\n",
            " samples avg       0.18      0.16      0.17       394\n",
            "\n",
            "Macro F1 Score for Test Set: 0.2546593275123032\n",
            "\n",
            "Summary for Random Forest:\n",
            "  Train Macro F1 Score: 0.8616289787433558\n",
            "  Test Macro F1 Score:  0.2546593275123032\n",
            "==============================================\n",
            "\n",
            "Example prediction using SVM:\n",
            "anger: 0\n",
            "disgust: 0\n",
            "fear: 0\n",
            "joy: 0\n",
            "sadness: 0\n",
            "surprise: 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#marathi"
      ],
      "metadata": {
        "id": "_LSA4FRbWFB5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Import classifiers and wrappers\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.multioutput import MultiOutputClassifier\n",
        "\n",
        "# Evaluation metrics\n",
        "from sklearn.metrics import classification_report, f1_score\n",
        "\n",
        "# For generating text embeddings\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# For scaling the embeddings\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# -------------------------------\n",
        "# 1. Load Data and Generate Embeddings\n",
        "# -------------------------------\n",
        "\n",
        "# Load datasets\n",
        "train_df = pd.read_csv(\"/content/mar.csv\")\n",
        "test_df = pd.read_csv(\"/content/mar_dev.csv\")\n",
        "\n",
        "# Load the LaBSE model for generating embeddings\n",
        "labse_model = SentenceTransformer('intfloat/multilingual-e5-large')\n",
        "\n",
        "def generate_embeddings(texts):\n",
        "    \"\"\"\n",
        "    Given a list or array of texts, generate and return their normalized embeddings.\n",
        "    \"\"\"\n",
        "    return np.array(labse_model.encode(texts, convert_to_numpy=True, normalize_embeddings=True))\n",
        "\n",
        "# Get text data\n",
        "train_texts = train_df['text'].values\n",
        "test_texts = test_df['text'].values\n",
        "\n",
        "# Generate embeddings for train and test sets\n",
        "train_embeddings = generate_embeddings(train_texts)\n",
        "test_embeddings = generate_embeddings(test_texts)\n",
        "\n",
        "# -------------------------------\n",
        "# 2. Prepare Labels and Scale Features\n",
        "# -------------------------------\n",
        "\n",
        "# Define the emotion label columns\n",
        "label_columns = ['anger', 'disgust', 'fear', 'joy', 'sadness', 'surprise']\n",
        "\n",
        "# Extract label arrays\n",
        "train_labels = train_df[label_columns].values\n",
        "test_labels = test_df[label_columns].values\n",
        "\n",
        "# Scale the embeddings (feature standardization)\n",
        "scaler = StandardScaler()\n",
        "train_embeddings_scaled = scaler.fit_transform(train_embeddings)\n",
        "test_embeddings_scaled = scaler.transform(test_embeddings)\n",
        "\n",
        "# -------------------------------\n",
        "# 3. Define, Train, and Evaluate Models\n",
        "# -------------------------------\n",
        "\n",
        "# Define the four models. For Random Forest we add some regularization\n",
        "# by limiting the maximum tree depth and controlling the minimum samples needed for splits.\n",
        "models = {\n",
        "    \"SVM\": MultiOutputClassifier(SVC(kernel='rbf', probability=True, random_state=42)),\n",
        "    \"Naive Bayes\": MultiOutputClassifier(GaussianNB()),\n",
        "    \"Logistic Regression\": MultiOutputClassifier(\n",
        "        LogisticRegression(max_iter=1000, random_state=42)\n",
        "    ),\n",
        "    \"Random Forest\": MultiOutputClassifier(\n",
        "        RandomForestClassifier(max_depth=10, min_samples_split=5, random_state=42)\n",
        "    )\n",
        "}\n",
        "\n",
        "def evaluate_model(model, embeddings, labels, dataset_name, model_name):\n",
        "    \"\"\"\n",
        "    Predict labels using the given model and embeddings, then print the classification report\n",
        "    and macro F1 score.\n",
        "    \"\"\"\n",
        "    predictions = model.predict(embeddings)\n",
        "    print(f\"\\nModel: {model_name} - {dataset_name}\")\n",
        "    print(\"Classification Report:\")\n",
        "    print(classification_report(labels, predictions, target_names=label_columns))\n",
        "\n",
        "    macro_f1 = f1_score(labels, predictions, average='macro')\n",
        "    print(f\"Macro F1 Score for {dataset_name}: {macro_f1}\\n\")\n",
        "    return macro_f1\n",
        "\n",
        "# Train and evaluate each model on both the training and testing sets.\n",
        "for model_name, clf in models.items():\n",
        "    print(\"==============================================\")\n",
        "    print(f\"Training {model_name}...\")\n",
        "    clf.fit(train_embeddings_scaled, train_labels)\n",
        "\n",
        "    # Evaluate on the Training Set\n",
        "    print(\"Evaluation on Train Set:\")\n",
        "    train_macro_f1 = evaluate_model(clf, train_embeddings_scaled, train_labels, \"Train Set\", model_name)\n",
        "\n",
        "    # Evaluate on the Testing Set\n",
        "    print(\"Evaluation on Test Set:\")\n",
        "    test_macro_f1 = evaluate_model(clf, test_embeddings_scaled, test_labels, \"Test Set\", model_name)\n",
        "\n",
        "    print(f\"Summary for {model_name}:\")\n",
        "    print(f\"  Train Macro F1 Score: {train_macro_f1}\")\n",
        "    print(f\"  Test Macro F1 Score:  {test_macro_f1}\")\n",
        "    print(\"==============================================\\n\")\n",
        "\n",
        "# -------------------------------\n",
        "# 4. Predict Emotions for New Text\n",
        "# -------------------------------\n",
        "\n",
        "def predict_emotions(text, model, scaler):\n",
        "    \"\"\"\n",
        "    Given a new text string, generate its embedding, scale it, and predict emotion labels.\n",
        "    Returns a dictionary mapping each emotion to its predicted label (0 or 1).\n",
        "    \"\"\"\n",
        "    # Generate embedding for the new text\n",
        "    embedding = generate_embeddings([text])\n",
        "    embedding_scaled = scaler.transform(embedding)\n",
        "\n",
        "    # Get predictions\n",
        "    predictions = model.predict(embedding_scaled)\n",
        "\n",
        "    # Map each label to its corresponding emotion\n",
        "    emotion_preds = {emotion: int(pred) for emotion, pred in zip(label_columns, predictions[0])}\n",
        "    return emotion_preds\n",
        "\n",
        "# Example usage for prediction using the SVM model (you can choose any of the trained models)\n",
        "example_text = \"यह एक उदाहरण वाक्य है\"\n",
        "print(\"Example prediction using SVM:\")\n",
        "svm_model = models[\"SVM\"]\n",
        "example_prediction = predict_emotions(example_text, svm_model, scaler)\n",
        "for emotion, pred in example_prediction.items():\n",
        "    print(f\"{emotion}: {pred}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "14W-yxSXWIaD",
        "outputId": "4e463e07-fd25-4545-b0b3-299a2434c931"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==============================================\n",
            "Training SVM...\n",
            "Evaluation on Train Set:\n",
            "\n",
            "Model: SVM - Train Set\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       anger       1.00      0.82      0.90       350\n",
            "     disgust       1.00      0.85      0.92       299\n",
            "        fear       1.00      0.92      0.96       382\n",
            "         joy       1.00      0.93      0.96       461\n",
            "     sadness       0.99      0.87      0.93       431\n",
            "    surprise       1.00      0.94      0.97       311\n",
            "\n",
            "   micro avg       1.00      0.89      0.94      2234\n",
            "   macro avg       1.00      0.89      0.94      2234\n",
            "weighted avg       1.00      0.89      0.94      2234\n",
            " samples avg       0.78      0.76      0.76      2234\n",
            "\n",
            "Macro F1 Score for Train Set: 0.9392631884249765\n",
            "\n",
            "Evaluation on Test Set:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model: SVM - Test Set\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       anger       1.00      0.57      0.73        14\n",
            "     disgust       1.00      0.73      0.84        11\n",
            "        fear       1.00      0.87      0.93        15\n",
            "         joy       1.00      0.89      0.94        19\n",
            "     sadness       0.93      0.76      0.84        17\n",
            "    surprise       1.00      0.92      0.96        12\n",
            "\n",
            "   micro avg       0.99      0.80      0.88        88\n",
            "   macro avg       0.99      0.79      0.87        88\n",
            "weighted avg       0.99      0.80      0.88        88\n",
            " samples avg       0.69      0.67      0.67        88\n",
            "\n",
            "Macro F1 Score for Test Set: 0.8729375466660474\n",
            "\n",
            "Summary for SVM:\n",
            "  Train Macro F1 Score: 0.9392631884249765\n",
            "  Test Macro F1 Score:  0.8729375466660474\n",
            "==============================================\n",
            "\n",
            "==============================================\n",
            "Training Naive Bayes...\n",
            "Evaluation on Train Set:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model: Naive Bayes - Train Set\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       anger       0.39      0.81      0.53       350\n",
            "     disgust       0.40      0.82      0.54       299\n",
            "        fear       0.69      0.83      0.75       382\n",
            "         joy       0.67      0.92      0.78       461\n",
            "     sadness       0.41      0.81      0.55       431\n",
            "    surprise       0.78      0.86      0.82       311\n",
            "\n",
            "   micro avg       0.52      0.85      0.64      2234\n",
            "   macro avg       0.56      0.84      0.66      2234\n",
            "weighted avg       0.56      0.85      0.66      2234\n",
            " samples avg       0.52      0.72      0.57      2234\n",
            "\n",
            "Macro F1 Score for Train Set: 0.6596436233523213\n",
            "\n",
            "Evaluation on Test Set:\n",
            "\n",
            "Model: Naive Bayes - Test Set\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       anger       0.41      0.86      0.56        14\n",
            "     disgust       0.36      0.73      0.48        11\n",
            "        fear       0.72      0.87      0.79        15\n",
            "         joy       0.86      1.00      0.93        19\n",
            "     sadness       0.36      0.71      0.48        17\n",
            "    surprise       0.80      1.00      0.89        12\n",
            "\n",
            "   micro avg       0.55      0.86      0.67        88\n",
            "   macro avg       0.59      0.86      0.69        88\n",
            "weighted avg       0.60      0.86      0.70        88\n",
            " samples avg       0.50      0.70      0.56        88\n",
            "\n",
            "Macro F1 Score for Test Set: 0.6877641607987609\n",
            "\n",
            "Summary for Naive Bayes:\n",
            "  Train Macro F1 Score: 0.6596436233523213\n",
            "  Test Macro F1 Score:  0.6877641607987609\n",
            "==============================================\n",
            "\n",
            "==============================================\n",
            "Training Logistic Regression...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation on Train Set:\n",
            "\n",
            "Model: Logistic Regression - Train Set\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       anger       1.00      1.00      1.00       350\n",
            "     disgust       1.00      1.00      1.00       299\n",
            "        fear       1.00      1.00      1.00       382\n",
            "         joy       1.00      1.00      1.00       461\n",
            "     sadness       1.00      1.00      1.00       431\n",
            "    surprise       1.00      1.00      1.00       311\n",
            "\n",
            "   micro avg       1.00      1.00      1.00      2234\n",
            "   macro avg       1.00      1.00      1.00      2234\n",
            "weighted avg       1.00      1.00      1.00      2234\n",
            " samples avg       0.83      0.83      0.83      2234\n",
            "\n",
            "Macro F1 Score for Train Set: 1.0\n",
            "\n",
            "Evaluation on Test Set:\n",
            "\n",
            "Model: Logistic Regression - Test Set\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       anger       0.73      0.79      0.76        14\n",
            "     disgust       1.00      0.91      0.95        11\n",
            "        fear       1.00      0.93      0.97        15\n",
            "         joy       0.88      0.79      0.83        19\n",
            "     sadness       0.71      0.71      0.71        17\n",
            "    surprise       0.85      0.92      0.88        12\n",
            "\n",
            "   micro avg       0.85      0.83      0.84        88\n",
            "   macro avg       0.86      0.84      0.85        88\n",
            "weighted avg       0.85      0.83      0.84        88\n",
            " samples avg       0.67      0.69      0.67        88\n",
            "\n",
            "Macro F1 Score for Test Set: 0.8492890949483242\n",
            "\n",
            "Summary for Logistic Regression:\n",
            "  Train Macro F1 Score: 1.0\n",
            "  Test Macro F1 Score:  0.8492890949483242\n",
            "==============================================\n",
            "\n",
            "==============================================\n",
            "Training Random Forest...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation on Train Set:\n",
            "\n",
            "Model: Random Forest - Train Set\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       anger       1.00      0.94      0.97       350\n",
            "     disgust       1.00      0.85      0.92       299\n",
            "        fear       1.00      0.93      0.96       382\n",
            "         joy       1.00      0.98      0.99       461\n",
            "     sadness       1.00      0.90      0.95       431\n",
            "    surprise       1.00      0.92      0.96       311\n",
            "\n",
            "   micro avg       1.00      0.92      0.96      2234\n",
            "   macro avg       1.00      0.92      0.96      2234\n",
            "weighted avg       1.00      0.92      0.96      2234\n",
            " samples avg       0.79      0.77      0.78      2234\n",
            "\n",
            "Macro F1 Score for Train Set: 0.9577887100148245\n",
            "\n",
            "Evaluation on Test Set:\n",
            "\n",
            "Model: Random Forest - Test Set\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       anger       1.00      0.29      0.44        14\n",
            "     disgust       1.00      0.45      0.62        11\n",
            "        fear       1.00      0.33      0.50        15\n",
            "         joy       1.00      0.68      0.81        19\n",
            "     sadness       1.00      0.12      0.21        17\n",
            "    surprise       1.00      0.33      0.50        12\n",
            "\n",
            "   micro avg       1.00      0.38      0.55        88\n",
            "   macro avg       1.00      0.37      0.52        88\n",
            "weighted avg       1.00      0.38      0.52        88\n",
            " samples avg       0.33      0.33      0.33        88\n",
            "\n",
            "Macro F1 Score for Test Set: 0.5154117933723197\n",
            "\n",
            "Summary for Random Forest:\n",
            "  Train Macro F1 Score: 0.9577887100148245\n",
            "  Test Macro F1 Score:  0.5154117933723197\n",
            "==============================================\n",
            "\n",
            "Example prediction using SVM:\n",
            "anger: 0\n",
            "disgust: 0\n",
            "fear: 0\n",
            "joy: 0\n",
            "sadness: 0\n",
            "surprise: 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# oromo"
      ],
      "metadata": {
        "id": "G126FZ0LWWMb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Import classifiers and wrappers\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.multioutput import MultiOutputClassifier\n",
        "\n",
        "# Evaluation metrics\n",
        "from sklearn.metrics import classification_report, f1_score\n",
        "\n",
        "# For generating text embeddings\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# For scaling the embeddings\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# -------------------------------\n",
        "# 1. Load Data and Generate Embeddings\n",
        "# -------------------------------\n",
        "\n",
        "# Load datasets\n",
        "train_df = pd.read_csv(\"/content/orm.csv\")\n",
        "test_df = pd.read_csv(\"/content/orm_dev.csv\")\n",
        "\n",
        "# Load the LaBSE model for generating embeddings\n",
        "labse_model = SentenceTransformer('intfloat/multilingual-e5-large')\n",
        "\n",
        "def generate_embeddings(texts):\n",
        "    \"\"\"\n",
        "    Given a list or array of texts, generate and return their normalized embeddings.\n",
        "    \"\"\"\n",
        "    return np.array(labse_model.encode(texts, convert_to_numpy=True, normalize_embeddings=True))\n",
        "\n",
        "# Get text data\n",
        "train_texts = train_df['text'].values\n",
        "test_texts = test_df['text'].values\n",
        "\n",
        "# Generate embeddings for train and test sets\n",
        "train_embeddings = generate_embeddings(train_texts)\n",
        "test_embeddings = generate_embeddings(test_texts)\n",
        "\n",
        "# -------------------------------\n",
        "# 2. Prepare Labels and Scale Features\n",
        "# -------------------------------\n",
        "\n",
        "# Define the emotion label columns\n",
        "label_columns = ['anger', 'disgust', 'fear', 'joy', 'sadness', 'surprise']\n",
        "\n",
        "# Extract label arrays\n",
        "train_labels = train_df[label_columns].values\n",
        "test_labels = test_df[label_columns].values\n",
        "\n",
        "# Scale the embeddings (feature standardization)\n",
        "scaler = StandardScaler()\n",
        "train_embeddings_scaled = scaler.fit_transform(train_embeddings)\n",
        "test_embeddings_scaled = scaler.transform(test_embeddings)\n",
        "\n",
        "# -------------------------------\n",
        "# 3. Define, Train, and Evaluate Models\n",
        "# -------------------------------\n",
        "\n",
        "# Define the four models. For Random Forest we add some regularization\n",
        "# by limiting the maximum tree depth and controlling the minimum samples needed for splits.\n",
        "models = {\n",
        "    \"SVM\": MultiOutputClassifier(SVC(kernel='rbf', probability=True, random_state=42)),\n",
        "    \"Naive Bayes\": MultiOutputClassifier(GaussianNB()),\n",
        "    \"Logistic Regression\": MultiOutputClassifier(\n",
        "        LogisticRegression(max_iter=1000, random_state=42)\n",
        "    ),\n",
        "    \"Random Forest\": MultiOutputClassifier(\n",
        "        RandomForestClassifier(max_depth=10, min_samples_split=5, random_state=42)\n",
        "    )\n",
        "}\n",
        "\n",
        "def evaluate_model(model, embeddings, labels, dataset_name, model_name):\n",
        "    \"\"\"\n",
        "    Predict labels using the given model and embeddings, then print the classification report\n",
        "    and macro F1 score.\n",
        "    \"\"\"\n",
        "    predictions = model.predict(embeddings)\n",
        "    print(f\"\\nModel: {model_name} - {dataset_name}\")\n",
        "    print(\"Classification Report:\")\n",
        "    print(classification_report(labels, predictions, target_names=label_columns))\n",
        "\n",
        "    macro_f1 = f1_score(labels, predictions, average='macro')\n",
        "    print(f\"Macro F1 Score for {dataset_name}: {macro_f1}\\n\")\n",
        "    return macro_f1\n",
        "\n",
        "# Train and evaluate each model on both the training and testing sets.\n",
        "for model_name, clf in models.items():\n",
        "    print(\"==============================================\")\n",
        "    print(f\"Training {model_name}...\")\n",
        "    clf.fit(train_embeddings_scaled, train_labels)\n",
        "\n",
        "    # Evaluate on the Training Set\n",
        "    print(\"Evaluation on Train Set:\")\n",
        "    train_macro_f1 = evaluate_model(clf, train_embeddings_scaled, train_labels, \"Train Set\", model_name)\n",
        "\n",
        "    # Evaluate on the Testing Set\n",
        "    print(\"Evaluation on Test Set:\")\n",
        "    test_macro_f1 = evaluate_model(clf, test_embeddings_scaled, test_labels, \"Test Set\", model_name)\n",
        "\n",
        "    print(f\"Summary for {model_name}:\")\n",
        "    print(f\"  Train Macro F1 Score: {train_macro_f1}\")\n",
        "    print(f\"  Test Macro F1 Score:  {test_macro_f1}\")\n",
        "    print(\"==============================================\\n\")\n",
        "\n",
        "# -------------------------------\n",
        "# 4. Predict Emotions for New Text\n",
        "# -------------------------------\n",
        "\n",
        "def predict_emotions(text, model, scaler):\n",
        "    \"\"\"\n",
        "    Given a new text string, generate its embedding, scale it, and predict emotion labels.\n",
        "    Returns a dictionary mapping each emotion to its predicted label (0 or 1).\n",
        "    \"\"\"\n",
        "    # Generate embedding for the new text\n",
        "    embedding = generate_embeddings([text])\n",
        "    embedding_scaled = scaler.transform(embedding)\n",
        "\n",
        "    # Get predictions\n",
        "    predictions = model.predict(embedding_scaled)\n",
        "\n",
        "    # Map each label to its corresponding emotion\n",
        "    emotion_preds = {emotion: int(pred) for emotion, pred in zip(label_columns, predictions[0])}\n",
        "    return emotion_preds\n",
        "\n",
        "# Example usage for prediction using the SVM model (you can choose any of the trained models)\n",
        "example_text = \"यह एक उदाहरण वाक्य है\"\n",
        "print(\"Example prediction using SVM:\")\n",
        "svm_model = models[\"SVM\"]\n",
        "example_prediction = predict_emotions(example_text, svm_model, scaler)\n",
        "for emotion, pred in example_prediction.items():\n",
        "    print(f\"{emotion}: {pred}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "orYx-l_CWZqd",
        "outputId": "574159cf-2e7e-4e94-dbef-f95230bbd03d"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==============================================\n",
            "Training SVM...\n",
            "Evaluation on Train Set:\n",
            "\n",
            "Model: SVM - Train Set\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       anger       0.98      0.68      0.81       646\n",
            "     disgust       0.99      0.67      0.80       557\n",
            "        fear       1.00      0.05      0.09       123\n",
            "         joy       0.98      0.91      0.95      1091\n",
            "     sadness       1.00      0.42      0.59       298\n",
            "    surprise       1.00      0.57      0.72       129\n",
            "\n",
            "   micro avg       0.99      0.71      0.82      2844\n",
            "   macro avg       0.99      0.55      0.66      2844\n",
            "weighted avg       0.99      0.71      0.80      2844\n",
            " samples avg       0.57      0.55      0.56      2844\n",
            "\n",
            "Macro F1 Score for Train Set: 0.6588929298423242\n",
            "\n",
            "Evaluation on Test Set:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model: SVM - Test Set\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       anger       0.52      0.22      0.31       108\n",
            "     disgust       0.79      0.20      0.32        94\n",
            "        fear       0.00      0.00      0.00        21\n",
            "         joy       0.83      0.66      0.74       183\n",
            "     sadness       1.00      0.02      0.04        52\n",
            "    surprise       0.67      0.15      0.24        27\n",
            "\n",
            "   micro avg       0.76      0.35      0.48       485\n",
            "   macro avg       0.64      0.21      0.28       485\n",
            "weighted avg       0.73      0.35      0.43       485\n",
            " samples avg       0.29      0.28      0.28       485\n",
            "\n",
            "Macro F1 Score for Test Set: 0.2752811965871705\n",
            "\n",
            "Summary for SVM:\n",
            "  Train Macro F1 Score: 0.6588929298423242\n",
            "  Test Macro F1 Score:  0.2752811965871705\n",
            "==============================================\n",
            "\n",
            "==============================================\n",
            "Training Naive Bayes...\n",
            "Evaluation on Train Set:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model: Naive Bayes - Train Set\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       anger       0.36      0.79      0.50       646\n",
            "     disgust       0.41      0.78      0.53       557\n",
            "        fear       0.11      0.77      0.20       123\n",
            "         joy       0.63      0.73      0.68      1091\n",
            "     sadness       0.24      0.74      0.36       298\n",
            "    surprise       0.24      0.71      0.36       129\n",
            "\n",
            "   micro avg       0.37      0.76      0.49      2844\n",
            "   macro avg       0.33      0.76      0.44      2844\n",
            "weighted avg       0.44      0.76      0.54      2844\n",
            " samples avg       0.39      0.58      0.44      2844\n",
            "\n",
            "Macro F1 Score for Train Set: 0.43725256017011377\n",
            "\n",
            "Evaluation on Test Set:\n",
            "\n",
            "Model: Naive Bayes - Test Set\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       anger       0.34      0.79      0.47       108\n",
            "     disgust       0.35      0.66      0.45        94\n",
            "        fear       0.08      0.57      0.14        21\n",
            "         joy       0.61      0.67      0.64       183\n",
            "     sadness       0.24      0.75      0.36        52\n",
            "    surprise       0.25      0.56      0.34        27\n",
            "\n",
            "   micro avg       0.33      0.69      0.45       485\n",
            "   macro avg       0.31      0.67      0.40       485\n",
            "weighted avg       0.42      0.69      0.50       485\n",
            " samples avg       0.35      0.52      0.39       485\n",
            "\n",
            "Macro F1 Score for Test Set: 0.4008128920917499\n",
            "\n",
            "Summary for Naive Bayes:\n",
            "  Train Macro F1 Score: 0.43725256017011377\n",
            "  Test Macro F1 Score:  0.4008128920917499\n",
            "==============================================\n",
            "\n",
            "==============================================\n",
            "Training Logistic Regression...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation on Train Set:\n",
            "\n",
            "Model: Logistic Regression - Train Set\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       anger       0.93      0.86      0.89       646\n",
            "     disgust       0.95      0.89      0.92       557\n",
            "        fear       1.00      1.00      1.00       123\n",
            "         joy       0.98      0.95      0.96      1091\n",
            "     sadness       1.00      0.99      1.00       298\n",
            "    surprise       1.00      1.00      1.00       129\n",
            "\n",
            "   micro avg       0.97      0.93      0.95      2844\n",
            "   macro avg       0.98      0.95      0.96      2844\n",
            "weighted avg       0.96      0.93      0.95      2844\n",
            " samples avg       0.71      0.71      0.71      2844\n",
            "\n",
            "Macro F1 Score for Train Set: 0.9622605375350397\n",
            "\n",
            "Evaluation on Test Set:\n",
            "\n",
            "Model: Logistic Regression - Test Set\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       anger       0.40      0.45      0.43       108\n",
            "     disgust       0.56      0.57      0.57        94\n",
            "        fear       0.24      0.24      0.24        21\n",
            "         joy       0.67      0.69      0.68       183\n",
            "     sadness       0.37      0.38      0.38        52\n",
            "    surprise       0.36      0.30      0.33        27\n",
            "\n",
            "   micro avg       0.52      0.54      0.53       485\n",
            "   macro avg       0.43      0.44      0.44       485\n",
            "weighted avg       0.52      0.54      0.53       485\n",
            " samples avg       0.39      0.43      0.39       485\n",
            "\n",
            "Macro F1 Score for Test Set: 0.43577037472605457\n",
            "\n",
            "Summary for Logistic Regression:\n",
            "  Train Macro F1 Score: 0.9622605375350397\n",
            "  Test Macro F1 Score:  0.43577037472605457\n",
            "==============================================\n",
            "\n",
            "==============================================\n",
            "Training Random Forest...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation on Train Set:\n",
            "\n",
            "Model: Random Forest - Train Set\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       anger       1.00      0.82      0.90       646\n",
            "     disgust       1.00      0.43      0.60       557\n",
            "        fear       0.00      0.00      0.00       123\n",
            "         joy       1.00      0.95      0.97      1091\n",
            "     sadness       1.00      0.15      0.27       298\n",
            "    surprise       1.00      0.16      0.27       129\n",
            "\n",
            "   micro avg       1.00      0.66      0.79      2844\n",
            "   macro avg       0.83      0.42      0.50      2844\n",
            "weighted avg       0.96      0.66      0.74      2844\n",
            " samples avg       0.53      0.51      0.51      2844\n",
            "\n",
            "Macro F1 Score for Train Set: 0.5014431883865434\n",
            "\n",
            "Evaluation on Test Set:\n",
            "\n",
            "Model: Random Forest - Test Set\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       anger       0.50      0.01      0.02       108\n",
            "     disgust       0.00      0.00      0.00        94\n",
            "        fear       0.00      0.00      0.00        21\n",
            "         joy       0.89      0.38      0.53       183\n",
            "     sadness       0.00      0.00      0.00        52\n",
            "    surprise       0.00      0.00      0.00        27\n",
            "\n",
            "   micro avg       0.88      0.15      0.25       485\n",
            "   macro avg       0.23      0.07      0.09       485\n",
            "weighted avg       0.45      0.15      0.21       485\n",
            " samples avg       0.12      0.12      0.12       485\n",
            "\n",
            "Macro F1 Score for Test Set: 0.09208882720333102\n",
            "\n",
            "Summary for Random Forest:\n",
            "  Train Macro F1 Score: 0.5014431883865434\n",
            "  Test Macro F1 Score:  0.09208882720333102\n",
            "==============================================\n",
            "\n",
            "Example prediction using SVM:\n",
            "anger: 0\n",
            "disgust: 0\n",
            "fear: 0\n",
            "joy: 0\n",
            "sadness: 0\n",
            "surprise: 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#romanian"
      ],
      "metadata": {
        "id": "ARUN7GSpWhDm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Import classifiers and wrappers\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.multioutput import MultiOutputClassifier\n",
        "\n",
        "# Evaluation metrics\n",
        "from sklearn.metrics import classification_report, f1_score\n",
        "\n",
        "# For generating text embeddings\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# For scaling the embeddings\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# -------------------------------\n",
        "# 1. Load Data and Generate Embeddings\n",
        "# -------------------------------\n",
        "\n",
        "# Load datasets\n",
        "train_df = pd.read_csv(\"/content/ron.csv\")\n",
        "test_df = pd.read_csv(\"/content/ron_dev.csv\")\n",
        "\n",
        "# Load the LaBSE model for generating embeddings\n",
        "labse_model = SentenceTransformer('intfloat/multilingual-e5-large')\n",
        "\n",
        "def generate_embeddings(texts):\n",
        "    \"\"\"\n",
        "    Given a list or array of texts, generate and return their normalized embeddings.\n",
        "    \"\"\"\n",
        "    return np.array(labse_model.encode(texts, convert_to_numpy=True, normalize_embeddings=True))\n",
        "\n",
        "# Get text data\n",
        "train_texts = train_df['text'].values\n",
        "test_texts = test_df['text'].values\n",
        "\n",
        "# Generate embeddings for train and test sets\n",
        "train_embeddings = generate_embeddings(train_texts)\n",
        "test_embeddings = generate_embeddings(test_texts)\n",
        "\n",
        "# -------------------------------\n",
        "# 2. Prepare Labels and Scale Features\n",
        "# -------------------------------\n",
        "\n",
        "# Define the emotion label columns\n",
        "label_columns = ['anger', 'disgust', 'fear', 'joy', 'sadness', 'surprise']\n",
        "\n",
        "# Extract label arrays\n",
        "train_labels = train_df[label_columns].values\n",
        "test_labels = test_df[label_columns].values\n",
        "\n",
        "# Scale the embeddings (feature standardization)\n",
        "scaler = StandardScaler()\n",
        "train_embeddings_scaled = scaler.fit_transform(train_embeddings)\n",
        "test_embeddings_scaled = scaler.transform(test_embeddings)\n",
        "\n",
        "# -------------------------------\n",
        "# 3. Define, Train, and Evaluate Models\n",
        "# -------------------------------\n",
        "\n",
        "# Define the four models. For Random Forest we add some regularization\n",
        "# by limiting the maximum tree depth and controlling the minimum samples needed for splits.\n",
        "models = {\n",
        "    \"SVM\": MultiOutputClassifier(SVC(kernel='rbf', probability=True, random_state=42)),\n",
        "    \"Naive Bayes\": MultiOutputClassifier(GaussianNB()),\n",
        "    \"Logistic Regression\": MultiOutputClassifier(\n",
        "        LogisticRegression(max_iter=1000, random_state=42)\n",
        "    ),\n",
        "    \"Random Forest\": MultiOutputClassifier(\n",
        "        RandomForestClassifier(max_depth=10, min_samples_split=5, random_state=42)\n",
        "    )\n",
        "}\n",
        "\n",
        "def evaluate_model(model, embeddings, labels, dataset_name, model_name):\n",
        "    \"\"\"\n",
        "    Predict labels using the given model and embeddings, then print the classification report\n",
        "    and macro F1 score.\n",
        "    \"\"\"\n",
        "    predictions = model.predict(embeddings)\n",
        "    print(f\"\\nModel: {model_name} - {dataset_name}\")\n",
        "    print(\"Classification Report:\")\n",
        "    print(classification_report(labels, predictions, target_names=label_columns))\n",
        "\n",
        "    macro_f1 = f1_score(labels, predictions, average='macro')\n",
        "    print(f\"Macro F1 Score for {dataset_name}: {macro_f1}\\n\")\n",
        "    return macro_f1\n",
        "\n",
        "# Train and evaluate each model on both the training and testing sets.\n",
        "for model_name, clf in models.items():\n",
        "    print(\"==============================================\")\n",
        "    print(f\"Training {model_name}...\")\n",
        "    clf.fit(train_embeddings_scaled, train_labels)\n",
        "\n",
        "    # Evaluate on the Training Set\n",
        "    print(\"Evaluation on Train Set:\")\n",
        "    train_macro_f1 = evaluate_model(clf, train_embeddings_scaled, train_labels, \"Train Set\", model_name)\n",
        "\n",
        "    # Evaluate on the Testing Set\n",
        "    print(\"Evaluation on Test Set:\")\n",
        "    test_macro_f1 = evaluate_model(clf, test_embeddings_scaled, test_labels, \"Test Set\", model_name)\n",
        "\n",
        "    print(f\"Summary for {model_name}:\")\n",
        "    print(f\"  Train Macro F1 Score: {train_macro_f1}\")\n",
        "    print(f\"  Test Macro F1 Score:  {test_macro_f1}\")\n",
        "    print(\"==============================================\\n\")\n",
        "\n",
        "# -------------------------------\n",
        "# 4. Predict Emotions for New Text\n",
        "# -------------------------------\n",
        "\n",
        "def predict_emotions(text, model, scaler):\n",
        "    \"\"\"\n",
        "    Given a new text string, generate its embedding, scale it, and predict emotion labels.\n",
        "    Returns a dictionary mapping each emotion to its predicted label (0 or 1).\n",
        "    \"\"\"\n",
        "    # Generate embedding for the new text\n",
        "    embedding = generate_embeddings([text])\n",
        "    embedding_scaled = scaler.transform(embedding)\n",
        "\n",
        "    # Get predictions\n",
        "    predictions = model.predict(embedding_scaled)\n",
        "\n",
        "    # Map each label to its corresponding emotion\n",
        "    emotion_preds = {emotion: int(pred) for emotion, pred in zip(label_columns, predictions[0])}\n",
        "    return emotion_preds\n",
        "\n",
        "# Example usage for prediction using the SVM model (you can choose any of the trained models)\n",
        "example_text = \"यह एक उदाहरण वाक्य है\"\n",
        "print(\"Example prediction using SVM:\")\n",
        "svm_model = models[\"SVM\"]\n",
        "example_prediction = predict_emotions(example_text, svm_model, scaler)\n",
        "for emotion, pred in example_prediction.items():\n",
        "    print(f\"{emotion}: {pred}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d2iK4PSrWm4C",
        "outputId": "84a6d35a-fde8-4692-b188-7c8d006b9d35"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==============================================\n",
            "Training SVM...\n",
            "Evaluation on Train Set:\n",
            "\n",
            "Model: SVM - Train Set\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       anger       1.00      0.90      0.95       210\n",
            "     disgust       1.00      0.97      0.98       297\n",
            "        fear       1.00      0.99      1.00       405\n",
            "         joy       1.00      0.99      1.00       462\n",
            "     sadness       0.99      0.98      0.99       379\n",
            "    surprise       1.00      0.87      0.93       457\n",
            "\n",
            "   micro avg       1.00      0.95      0.98      2210\n",
            "   macro avg       1.00      0.95      0.97      2210\n",
            "weighted avg       1.00      0.95      0.97      2210\n",
            " samples avg       0.91      0.88      0.89      2210\n",
            "\n",
            "Macro F1 Score for Train Set: 0.9736568172663679\n",
            "\n",
            "Evaluation on Test Set:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model: SVM - Test Set\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       anger       1.00      0.17      0.29        24\n",
            "     disgust       0.88      0.48      0.62        29\n",
            "        fear       0.78      0.78      0.78        41\n",
            "         joy       0.98      1.00      0.99        45\n",
            "     sadness       0.82      0.74      0.78        38\n",
            "    surprise       0.47      0.21      0.29        38\n",
            "\n",
            "   micro avg       0.83      0.61      0.70       215\n",
            "   macro avg       0.82      0.56      0.62       215\n",
            "weighted avg       0.81      0.61      0.66       215\n",
            " samples avg       0.67      0.58      0.60       215\n",
            "\n",
            "Macro F1 Score for Test Set: 0.6243536950854024\n",
            "\n",
            "Summary for SVM:\n",
            "  Train Macro F1 Score: 0.9736568172663679\n",
            "  Test Macro F1 Score:  0.6243536950854024\n",
            "==============================================\n",
            "\n",
            "==============================================\n",
            "Training Naive Bayes...\n",
            "Evaluation on Train Set:\n",
            "\n",
            "Model: Naive Bayes - Train Set\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       anger       0.39      0.93      0.55       210\n",
            "     disgust       0.45      0.90      0.60       297\n",
            "        fear       0.66      0.94      0.77       405\n",
            "         joy       0.98      0.90      0.94       462\n",
            "     sadness       0.57      0.89      0.69       379\n",
            "    surprise       0.63      0.72      0.67       457\n",
            "\n",
            "   micro avg       0.60      0.87      0.71      2210\n",
            "   macro avg       0.61      0.88      0.70      2210\n",
            "weighted avg       0.65      0.87      0.73      2210\n",
            " samples avg       0.64      0.80      0.67      2210\n",
            "\n",
            "Macro F1 Score for Train Set: 0.7041619841480696\n",
            "\n",
            "Evaluation on Test Set:\n",
            "\n",
            "Model: Naive Bayes - Test Set\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       anger       0.39      0.88      0.54        24\n",
            "     disgust       0.41      0.86      0.56        29\n",
            "        fear       0.64      0.93      0.76        41\n",
            "         joy       0.96      0.98      0.97        45\n",
            "     sadness       0.55      0.87      0.67        38\n",
            "    surprise       0.43      0.55      0.48        38\n",
            "\n",
            "   micro avg       0.55      0.85      0.67       215\n",
            "   macro avg       0.56      0.84      0.66       215\n",
            "weighted avg       0.59      0.85      0.69       215\n",
            " samples avg       0.60      0.74      0.62       215\n",
            "\n",
            "Macro F1 Score for Test Set: 0.6628796782491364\n",
            "\n",
            "Summary for Naive Bayes:\n",
            "  Train Macro F1 Score: 0.7041619841480696\n",
            "  Test Macro F1 Score:  0.6628796782491364\n",
            "==============================================\n",
            "\n",
            "==============================================\n",
            "Training Logistic Regression...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation on Train Set:\n",
            "\n",
            "Model: Logistic Regression - Train Set\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       anger       1.00      1.00      1.00       210\n",
            "     disgust       1.00      1.00      1.00       297\n",
            "        fear       1.00      1.00      1.00       405\n",
            "         joy       1.00      1.00      1.00       462\n",
            "     sadness       1.00      1.00      1.00       379\n",
            "    surprise       1.00      1.00      1.00       457\n",
            "\n",
            "   micro avg       1.00      1.00      1.00      2210\n",
            "   macro avg       1.00      1.00      1.00      2210\n",
            "weighted avg       1.00      1.00      1.00      2210\n",
            " samples avg       0.91      0.91      0.91      2210\n",
            "\n",
            "Macro F1 Score for Train Set: 1.0\n",
            "\n",
            "Evaluation on Test Set:\n",
            "\n",
            "Model: Logistic Regression - Test Set\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       anger       0.67      0.58      0.62        24\n",
            "     disgust       0.69      0.69      0.69        29\n",
            "        fear       0.71      0.78      0.74        41\n",
            "         joy       0.98      1.00      0.99        45\n",
            "     sadness       0.71      0.63      0.67        38\n",
            "    surprise       0.38      0.61      0.47        38\n",
            "\n",
            "   micro avg       0.67      0.73      0.70       215\n",
            "   macro avg       0.69      0.72      0.70       215\n",
            "weighted avg       0.70      0.73      0.71       215\n",
            " samples avg       0.64      0.69      0.63       215\n",
            "\n",
            "Macro F1 Score for Test Set: 0.6968548086545566\n",
            "\n",
            "Summary for Logistic Regression:\n",
            "  Train Macro F1 Score: 1.0\n",
            "  Test Macro F1 Score:  0.6968548086545566\n",
            "==============================================\n",
            "\n",
            "==============================================\n",
            "Training Random Forest...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation on Train Set:\n",
            "\n",
            "Model: Random Forest - Train Set\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       anger       1.00      0.97      0.99       210\n",
            "     disgust       1.00      0.99      1.00       297\n",
            "        fear       1.00      1.00      1.00       405\n",
            "         joy       1.00      1.00      1.00       462\n",
            "     sadness       1.00      0.99      1.00       379\n",
            "    surprise       1.00      1.00      1.00       457\n",
            "\n",
            "   micro avg       1.00      1.00      1.00      2210\n",
            "   macro avg       1.00      0.99      1.00      2210\n",
            "weighted avg       1.00      1.00      1.00      2210\n",
            " samples avg       0.91      0.91      0.91      2210\n",
            "\n",
            "Macro F1 Score for Train Set: 0.9965805608921552\n",
            "\n",
            "Evaluation on Test Set:\n",
            "\n",
            "Model: Random Forest - Test Set\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       anger       0.00      0.00      0.00        24\n",
            "     disgust       0.50      0.03      0.06        29\n",
            "        fear       0.83      0.59      0.69        41\n",
            "         joy       0.95      0.91      0.93        45\n",
            "     sadness       0.81      0.55      0.66        38\n",
            "    surprise       0.00      0.00      0.00        38\n",
            "\n",
            "   micro avg       0.86      0.40      0.55       215\n",
            "   macro avg       0.51      0.35      0.39       215\n",
            "weighted avg       0.57      0.40      0.45       215\n",
            " samples avg       0.59      0.44      0.47       215\n",
            "\n",
            "Macro F1 Score for Test Set: 0.3897164327607876\n",
            "\n",
            "Summary for Random Forest:\n",
            "  Train Macro F1 Score: 0.9965805608921552\n",
            "  Test Macro F1 Score:  0.3897164327607876\n",
            "==============================================\n",
            "\n",
            "Example prediction using SVM:\n",
            "anger: 0\n",
            "disgust: 0\n",
            "fear: 0\n",
            "joy: 0\n",
            "sadness: 0\n",
            "surprise: 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# rusian"
      ],
      "metadata": {
        "id": "q6Br_mlXW7WD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Import classifiers and wrappers\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.multioutput import MultiOutputClassifier\n",
        "\n",
        "# Evaluation metrics\n",
        "from sklearn.metrics import classification_report, f1_score\n",
        "\n",
        "# For generating text embeddings\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# For scaling the embeddings\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# -------------------------------\n",
        "# 1. Load Data and Generate Embeddings\n",
        "# -------------------------------\n",
        "\n",
        "# Load datasets\n",
        "train_df = pd.read_csv(\"/content/rus.csv\")\n",
        "test_df = pd.read_csv(\"/content/rus_dev.csv\")\n",
        "\n",
        "# Load the LaBSE model for generating embeddings\n",
        "labse_model = SentenceTransformer('intfloat/multilingual-e5-large')\n",
        "\n",
        "def generate_embeddings(texts):\n",
        "    \"\"\"\n",
        "    Given a list or array of texts, generate and return their normalized embeddings.\n",
        "    \"\"\"\n",
        "    return np.array(labse_model.encode(texts, convert_to_numpy=True, normalize_embeddings=True))\n",
        "\n",
        "# Get text data\n",
        "train_texts = train_df['text'].values\n",
        "test_texts = test_df['text'].values\n",
        "\n",
        "# Generate embeddings for train and test sets\n",
        "train_embeddings = generate_embeddings(train_texts)\n",
        "test_embeddings = generate_embeddings(test_texts)\n",
        "\n",
        "# -------------------------------\n",
        "# 2. Prepare Labels and Scale Features\n",
        "# -------------------------------\n",
        "\n",
        "# Define the emotion label columns\n",
        "label_columns = ['anger', 'disgust', 'fear', 'joy', 'sadness', 'surprise']\n",
        "\n",
        "# Extract label arrays\n",
        "train_labels = train_df[label_columns].values\n",
        "test_labels = test_df[label_columns].values\n",
        "\n",
        "# Scale the embeddings (feature standardization)\n",
        "scaler = StandardScaler()\n",
        "train_embeddings_scaled = scaler.fit_transform(train_embeddings)\n",
        "test_embeddings_scaled = scaler.transform(test_embeddings)\n",
        "\n",
        "# -------------------------------\n",
        "# 3. Define, Train, and Evaluate Models\n",
        "# -------------------------------\n",
        "\n",
        "# Define the four models. For Random Forest we add some regularization\n",
        "# by limiting the maximum tree depth and controlling the minimum samples needed for splits.\n",
        "models = {\n",
        "    \"SVM\": MultiOutputClassifier(SVC(kernel='rbf', probability=True, random_state=42)),\n",
        "    \"Naive Bayes\": MultiOutputClassifier(GaussianNB()),\n",
        "    \"Logistic Regression\": MultiOutputClassifier(\n",
        "        LogisticRegression(max_iter=1000, random_state=42)\n",
        "    ),\n",
        "    \"Random Forest\": MultiOutputClassifier(\n",
        "        RandomForestClassifier(max_depth=10, min_samples_split=5, random_state=42)\n",
        "    )\n",
        "}\n",
        "\n",
        "def evaluate_model(model, embeddings, labels, dataset_name, model_name):\n",
        "    \"\"\"\n",
        "    Predict labels using the given model and embeddings, then print the classification report\n",
        "    and macro F1 score.\n",
        "    \"\"\"\n",
        "    predictions = model.predict(embeddings)\n",
        "    print(f\"\\nModel: {model_name} - {dataset_name}\")\n",
        "    print(\"Classification Report:\")\n",
        "    print(classification_report(labels, predictions, target_names=label_columns))\n",
        "\n",
        "    macro_f1 = f1_score(labels, predictions, average='macro')\n",
        "    print(f\"Macro F1 Score for {dataset_name}: {macro_f1}\\n\")\n",
        "    return macro_f1\n",
        "\n",
        "# Train and evaluate each model on both the training and testing sets.\n",
        "for model_name, clf in models.items():\n",
        "    print(\"==============================================\")\n",
        "    print(f\"Training {model_name}...\")\n",
        "    clf.fit(train_embeddings_scaled, train_labels)\n",
        "\n",
        "    # Evaluate on the Training Set\n",
        "    print(\"Evaluation on Train Set:\")\n",
        "    train_macro_f1 = evaluate_model(clf, train_embeddings_scaled, train_labels, \"Train Set\", model_name)\n",
        "\n",
        "    # Evaluate on the Testing Set\n",
        "    print(\"Evaluation on Test Set:\")\n",
        "    test_macro_f1 = evaluate_model(clf, test_embeddings_scaled, test_labels, \"Test Set\", model_name)\n",
        "\n",
        "    print(f\"Summary for {model_name}:\")\n",
        "    print(f\"  Train Macro F1 Score: {train_macro_f1}\")\n",
        "    print(f\"  Test Macro F1 Score:  {test_macro_f1}\")\n",
        "    print(\"==============================================\\n\")\n",
        "\n",
        "# -------------------------------\n",
        "# 4. Predict Emotions for New Text\n",
        "# -------------------------------\n",
        "\n",
        "def predict_emotions(text, model, scaler):\n",
        "    \"\"\"\n",
        "    Given a new text string, generate its embedding, scale it, and predict emotion labels.\n",
        "    Returns a dictionary mapping each emotion to its predicted label (0 or 1).\n",
        "    \"\"\"\n",
        "    # Generate embedding for the new text\n",
        "    embedding = generate_embeddings([text])\n",
        "    embedding_scaled = scaler.transform(embedding)\n",
        "\n",
        "    # Get predictions\n",
        "    predictions = model.predict(embedding_scaled)\n",
        "\n",
        "    # Map each label to its corresponding emotion\n",
        "    emotion_preds = {emotion: int(pred) for emotion, pred in zip(label_columns, predictions[0])}\n",
        "    return emotion_preds\n",
        "\n",
        "# Example usage for prediction using the SVM model (you can choose any of the trained models)\n",
        "example_text = \"यह एक उदाहरण वाक्य है\"\n",
        "print(\"Example prediction using SVM:\")\n",
        "svm_model = models[\"SVM\"]\n",
        "example_prediction = predict_emotions(example_text, svm_model, scaler)\n",
        "for emotion, pred in example_prediction.items():\n",
        "    print(f\"{emotion}: {pred}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jv9BHTZgXA36",
        "outputId": "bd0f95d4-6173-49bb-ae17-c07875565539"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==============================================\n",
            "Training SVM...\n",
            "Evaluation on Train Set:\n",
            "\n",
            "Model: SVM - Train Set\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       anger       0.99      0.95      0.97       543\n",
            "     disgust       1.00      0.89      0.94       273\n",
            "        fear       1.00      0.95      0.98       328\n",
            "         joy       0.99      0.97      0.98       555\n",
            "     sadness       1.00      0.90      0.95       421\n",
            "    surprise       1.00      0.90      0.95       355\n",
            "\n",
            "   micro avg       1.00      0.93      0.96      2475\n",
            "   macro avg       1.00      0.93      0.96      2475\n",
            "weighted avg       1.00      0.93      0.96      2475\n",
            " samples avg       0.78      0.76      0.77      2475\n",
            "\n",
            "Macro F1 Score for Train Set: 0.9597174444289424\n",
            "\n",
            "Evaluation on Test Set:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model: SVM - Test Set\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       anger       0.87      0.70      0.78        47\n",
            "     disgust       1.00      0.62      0.76        26\n",
            "        fear       0.95      0.86      0.90        21\n",
            "         joy       0.82      0.68      0.74        34\n",
            "     sadness       0.89      0.64      0.75        39\n",
            "    surprise       0.88      0.54      0.67        26\n",
            "\n",
            "   micro avg       0.89      0.67      0.76       193\n",
            "   macro avg       0.90      0.67      0.77       193\n",
            "weighted avg       0.89      0.67      0.76       193\n",
            " samples avg       0.63      0.61      0.61       193\n",
            "\n",
            "Macro F1 Score for Test Set: 0.7655410262323513\n",
            "\n",
            "Summary for SVM:\n",
            "  Train Macro F1 Score: 0.9597174444289424\n",
            "  Test Macro F1 Score:  0.7655410262323513\n",
            "==============================================\n",
            "\n",
            "==============================================\n",
            "Training Naive Bayes...\n",
            "Evaluation on Train Set:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model: Naive Bayes - Train Set\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       anger       0.63      0.89      0.74       543\n",
            "     disgust       0.52      0.90      0.66       273\n",
            "        fear       0.81      0.87      0.84       328\n",
            "         joy       0.79      0.89      0.84       555\n",
            "     sadness       0.63      0.88      0.73       421\n",
            "    surprise       0.58      0.82      0.68       355\n",
            "\n",
            "   micro avg       0.66      0.88      0.75      2475\n",
            "   macro avg       0.66      0.87      0.75      2475\n",
            "weighted avg       0.67      0.88      0.76      2475\n",
            " samples avg       0.58      0.72      0.62      2475\n",
            "\n",
            "Macro F1 Score for Train Set: 0.7485397694585038\n",
            "\n",
            "Evaluation on Test Set:\n",
            "\n",
            "Model: Naive Bayes - Test Set\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       anger       0.66      0.91      0.77        47\n",
            "     disgust       0.70      0.81      0.75        26\n",
            "        fear       0.67      0.95      0.78        21\n",
            "         joy       0.75      0.79      0.77        34\n",
            "     sadness       0.62      0.85      0.72        39\n",
            "    surprise       0.55      0.69      0.61        26\n",
            "\n",
            "   micro avg       0.66      0.84      0.74       193\n",
            "   macro avg       0.66      0.83      0.73       193\n",
            "weighted avg       0.66      0.84      0.74       193\n",
            " samples avg       0.61      0.73      0.64       193\n",
            "\n",
            "Macro F1 Score for Test Set: 0.7335267059415266\n",
            "\n",
            "Summary for Naive Bayes:\n",
            "  Train Macro F1 Score: 0.7485397694585038\n",
            "  Test Macro F1 Score:  0.7335267059415266\n",
            "==============================================\n",
            "\n",
            "==============================================\n",
            "Training Logistic Regression...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation on Train Set:\n",
            "\n",
            "Model: Logistic Regression - Train Set\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       anger       1.00      1.00      1.00       543\n",
            "     disgust       1.00      1.00      1.00       273\n",
            "        fear       1.00      1.00      1.00       328\n",
            "         joy       1.00      1.00      1.00       555\n",
            "     sadness       1.00      1.00      1.00       421\n",
            "    surprise       1.00      1.00      1.00       355\n",
            "\n",
            "   micro avg       1.00      1.00      1.00      2475\n",
            "   macro avg       1.00      1.00      1.00      2475\n",
            "weighted avg       1.00      1.00      1.00      2475\n",
            " samples avg       0.81      0.81      0.81      2475\n",
            "\n",
            "Macro F1 Score for Train Set: 1.0\n",
            "\n",
            "Evaluation on Test Set:\n",
            "\n",
            "Model: Logistic Regression - Test Set\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       anger       0.71      0.79      0.75        47\n",
            "     disgust       0.80      0.62      0.70        26\n",
            "        fear       0.83      0.95      0.89        21\n",
            "         joy       0.73      0.71      0.72        34\n",
            "     sadness       0.72      0.74      0.73        39\n",
            "    surprise       0.65      0.65      0.65        26\n",
            "\n",
            "   micro avg       0.73      0.74      0.74       193\n",
            "   macro avg       0.74      0.74      0.74       193\n",
            "weighted avg       0.73      0.74      0.74       193\n",
            " samples avg       0.63      0.66      0.63       193\n",
            "\n",
            "Macro F1 Score for Test Set: 0.739409514960078\n",
            "\n",
            "Summary for Logistic Regression:\n",
            "  Train Macro F1 Score: 1.0\n",
            "  Test Macro F1 Score:  0.739409514960078\n",
            "==============================================\n",
            "\n",
            "==============================================\n",
            "Training Random Forest...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation on Train Set:\n",
            "\n",
            "Model: Random Forest - Train Set\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       anger       1.00      0.98      0.99       543\n",
            "     disgust       1.00      0.77      0.87       273\n",
            "        fear       1.00      0.90      0.95       328\n",
            "         joy       1.00      0.98      0.99       555\n",
            "     sadness       1.00      0.93      0.96       421\n",
            "    surprise       1.00      0.76      0.86       355\n",
            "\n",
            "   micro avg       1.00      0.90      0.95      2475\n",
            "   macro avg       1.00      0.89      0.94      2475\n",
            "weighted avg       1.00      0.90      0.95      2475\n",
            " samples avg       0.76      0.74      0.75      2475\n",
            "\n",
            "Macro F1 Score for Train Set: 0.936800556234639\n",
            "\n",
            "Evaluation on Test Set:\n",
            "\n",
            "Model: Random Forest - Test Set\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       anger       1.00      0.26      0.41        47\n",
            "     disgust       0.00      0.00      0.00        26\n",
            "        fear       1.00      0.38      0.55        21\n",
            "         joy       1.00      0.15      0.26        34\n",
            "     sadness       1.00      0.10      0.19        39\n",
            "    surprise       1.00      0.12      0.21        26\n",
            "\n",
            "   micro avg       1.00      0.17      0.28       193\n",
            "   macro avg       0.83      0.17      0.27       193\n",
            "weighted avg       0.87      0.17      0.27       193\n",
            " samples avg       0.16      0.16      0.16       193\n",
            "\n",
            "Macro F1 Score for Test Set: 0.2679761864517141\n",
            "\n",
            "Summary for Random Forest:\n",
            "  Train Macro F1 Score: 0.936800556234639\n",
            "  Test Macro F1 Score:  0.2679761864517141\n",
            "==============================================\n",
            "\n",
            "Example prediction using SVM:\n",
            "anger: 0\n",
            "disgust: 0\n",
            "fear: 0\n",
            "joy: 0\n",
            "sadness: 0\n",
            "surprise: 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# spanish"
      ],
      "metadata": {
        "id": "qwgtw4h8XOui"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Import classifiers and wrappers\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.multioutput import MultiOutputClassifier\n",
        "\n",
        "# Evaluation metrics\n",
        "from sklearn.metrics import classification_report, f1_score\n",
        "\n",
        "# For generating text embeddings\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# For scaling the embeddings\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# -------------------------------\n",
        "# 1. Load Data and Generate Embeddings\n",
        "# -------------------------------\n",
        "\n",
        "# Load datasets\n",
        "train_df = pd.read_csv(\"/content/esp.csv\")\n",
        "test_df = pd.read_csv(\"/content/esp_dev.csv\")\n",
        "\n",
        "# Load the LaBSE model for generating embeddings\n",
        "labse_model = SentenceTransformer('intfloat/multilingual-e5-large')\n",
        "\n",
        "def generate_embeddings(texts):\n",
        "    \"\"\"\n",
        "    Given a list or array of texts, generate and return their normalized embeddings.\n",
        "    \"\"\"\n",
        "    return np.array(labse_model.encode(texts, convert_to_numpy=True, normalize_embeddings=True))\n",
        "\n",
        "# Get text data\n",
        "train_texts = train_df['text'].values\n",
        "test_texts = test_df['text'].values\n",
        "\n",
        "# Generate embeddings for train and test sets\n",
        "train_embeddings = generate_embeddings(train_texts)\n",
        "test_embeddings = generate_embeddings(test_texts)\n",
        "\n",
        "# -------------------------------\n",
        "# 2. Prepare Labels and Scale Features\n",
        "# -------------------------------\n",
        "\n",
        "# Define the emotion label columns\n",
        "label_columns = ['anger', 'disgust', 'fear', 'joy', 'sadness', 'surprise']\n",
        "\n",
        "# Extract label arrays\n",
        "train_labels = train_df[label_columns].values\n",
        "test_labels = test_df[label_columns].values\n",
        "\n",
        "# Scale the embeddings (feature standardization)\n",
        "scaler = StandardScaler()\n",
        "train_embeddings_scaled = scaler.fit_transform(train_embeddings)\n",
        "test_embeddings_scaled = scaler.transform(test_embeddings)\n",
        "\n",
        "# -------------------------------\n",
        "# 3. Define, Train, and Evaluate Models\n",
        "# -------------------------------\n",
        "\n",
        "# Define the four models. For Random Forest we add some regularization\n",
        "# by limiting the maximum tree depth and controlling the minimum samples needed for splits.\n",
        "models = {\n",
        "    \"SVM\": MultiOutputClassifier(SVC(kernel='rbf', probability=True, random_state=42)),\n",
        "    \"Naive Bayes\": MultiOutputClassifier(GaussianNB()),\n",
        "    \"Logistic Regression\": MultiOutputClassifier(\n",
        "        LogisticRegression(max_iter=1000, random_state=42)\n",
        "    ),\n",
        "    \"Random Forest\": MultiOutputClassifier(\n",
        "        RandomForestClassifier(max_depth=10, min_samples_split=5, random_state=42)\n",
        "    )\n",
        "}\n",
        "\n",
        "def evaluate_model(model, embeddings, labels, dataset_name, model_name):\n",
        "    \"\"\"\n",
        "    Predict labels using the given model and embeddings, then print the classification report\n",
        "    and macro F1 score.\n",
        "    \"\"\"\n",
        "    predictions = model.predict(embeddings)\n",
        "    print(f\"\\nModel: {model_name} - {dataset_name}\")\n",
        "    print(\"Classification Report:\")\n",
        "    print(classification_report(labels, predictions, target_names=label_columns))\n",
        "\n",
        "    macro_f1 = f1_score(labels, predictions, average='macro')\n",
        "    print(f\"Macro F1 Score for {dataset_name}: {macro_f1}\\n\")\n",
        "    return macro_f1\n",
        "\n",
        "# Train and evaluate each model on both the training and testing sets.\n",
        "for model_name, clf in models.items():\n",
        "    print(\"==============================================\")\n",
        "    print(f\"Training {model_name}...\")\n",
        "    clf.fit(train_embeddings_scaled, train_labels)\n",
        "\n",
        "    # Evaluate on the Training Set\n",
        "    print(\"Evaluation on Train Set:\")\n",
        "    train_macro_f1 = evaluate_model(clf, train_embeddings_scaled, train_labels, \"Train Set\", model_name)\n",
        "\n",
        "    # Evaluate on the Testing Set\n",
        "    print(\"Evaluation on Test Set:\")\n",
        "    test_macro_f1 = evaluate_model(clf, test_embeddings_scaled, test_labels, \"Test Set\", model_name)\n",
        "\n",
        "    print(f\"Summary for {model_name}:\")\n",
        "    print(f\"  Train Macro F1 Score: {train_macro_f1}\")\n",
        "    print(f\"  Test Macro F1 Score:  {test_macro_f1}\")\n",
        "    print(\"==============================================\\n\")\n",
        "\n",
        "# -------------------------------\n",
        "# 4. Predict Emotions for New Text\n",
        "# -------------------------------\n",
        "\n",
        "def predict_emotions(text, model, scaler):\n",
        "    \"\"\"\n",
        "    Given a new text string, generate its embedding, scale it, and predict emotion labels.\n",
        "    Returns a dictionary mapping each emotion to its predicted label (0 or 1).\n",
        "    \"\"\"\n",
        "    # Generate embedding for the new text\n",
        "    embedding = generate_embeddings([text])\n",
        "    embedding_scaled = scaler.transform(embedding)\n",
        "\n",
        "    # Get predictions\n",
        "    predictions = model.predict(embedding_scaled)\n",
        "\n",
        "    # Map each label to its corresponding emotion\n",
        "    emotion_preds = {emotion: int(pred) for emotion, pred in zip(label_columns, predictions[0])}\n",
        "    return emotion_preds\n",
        "\n",
        "# Example usage for prediction using the SVM model (you can choose any of the trained models)\n",
        "example_text = \"यह एक उदाहरण वाक्य है\"\n",
        "print(\"Example prediction using SVM:\")\n",
        "svm_model = models[\"SVM\"]\n",
        "example_prediction = predict_emotions(example_text, svm_model, scaler)\n",
        "for emotion, pred in example_prediction.items():\n",
        "    print(f\"{emotion}: {pred}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2IAVD5ZtXRkm",
        "outputId": "61ac7eb7-f292-4b34-b7fc-39aeb6af8018"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==============================================\n",
            "Training SVM...\n",
            "Evaluation on Train Set:\n",
            "\n",
            "Model: SVM - Train Set\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       anger       0.98      0.93      0.96       492\n",
            "     disgust       0.98      0.97      0.97       654\n",
            "        fear       0.98      0.94      0.96       317\n",
            "         joy       0.99      0.97      0.98       642\n",
            "     sadness       1.00      0.91      0.95       309\n",
            "    surprise       1.00      0.82      0.90       421\n",
            "\n",
            "   micro avg       0.99      0.93      0.96      2835\n",
            "   macro avg       0.99      0.92      0.95      2835\n",
            "weighted avg       0.99      0.93      0.96      2835\n",
            " samples avg       0.97      0.95      0.95      2835\n",
            "\n",
            "Macro F1 Score for Train Set: 0.9541033216494447\n",
            "\n",
            "Evaluation on Test Set:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model: SVM - Test Set\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       anger       0.63      0.65      0.64        34\n",
            "     disgust       0.85      0.72      0.78        65\n",
            "        fear       1.00      0.75      0.86        32\n",
            "         joy       0.88      0.83      0.86        54\n",
            "     sadness       1.00      0.68      0.81        31\n",
            "    surprise       0.87      0.33      0.47        40\n",
            "\n",
            "   micro avg       0.86      0.67      0.75       256\n",
            "   macro avg       0.87      0.66      0.74       256\n",
            "weighted avg       0.87      0.67      0.74       256\n",
            " samples avg       0.75      0.72      0.71       256\n",
            "\n",
            "Macro F1 Score for Test Set: 0.7359532979098197\n",
            "\n",
            "Summary for SVM:\n",
            "  Train Macro F1 Score: 0.9541033216494447\n",
            "  Test Macro F1 Score:  0.7359532979098197\n",
            "==============================================\n",
            "\n",
            "==============================================\n",
            "Training Naive Bayes...\n",
            "Evaluation on Train Set:\n",
            "\n",
            "Model: Naive Bayes - Train Set\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       anger       0.69      0.86      0.76       492\n",
            "     disgust       0.79      0.87      0.83       654\n",
            "        fear       0.91      0.87      0.89       317\n",
            "         joy       0.80      0.84      0.82       642\n",
            "     sadness       0.81      0.83      0.82       309\n",
            "    surprise       0.60      0.78      0.68       421\n",
            "\n",
            "   micro avg       0.75      0.84      0.80      2835\n",
            "   macro avg       0.77      0.84      0.80      2835\n",
            "weighted avg       0.76      0.84      0.80      2835\n",
            " samples avg       0.77      0.87      0.79      2835\n",
            "\n",
            "Macro F1 Score for Train Set: 0.8000074292389535\n",
            "\n",
            "Evaluation on Test Set:\n",
            "\n",
            "Model: Naive Bayes - Test Set\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       anger       0.53      0.82      0.64        34\n",
            "     disgust       0.78      0.75      0.77        65\n",
            "        fear       0.90      0.81      0.85        32\n",
            "         joy       0.80      0.81      0.81        54\n",
            "     sadness       0.77      0.87      0.82        31\n",
            "    surprise       0.56      0.62      0.59        40\n",
            "\n",
            "   micro avg       0.71      0.78      0.74       256\n",
            "   macro avg       0.72      0.78      0.75       256\n",
            "weighted avg       0.73      0.78      0.75       256\n",
            " samples avg       0.72      0.82      0.73       256\n",
            "\n",
            "Macro F1 Score for Test Set: 0.7459197898589555\n",
            "\n",
            "Summary for Naive Bayes:\n",
            "  Train Macro F1 Score: 0.8000074292389535\n",
            "  Test Macro F1 Score:  0.7459197898589555\n",
            "==============================================\n",
            "\n",
            "==============================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Logistic Regression...\n",
            "Evaluation on Train Set:\n",
            "\n",
            "Model: Logistic Regression - Train Set\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       anger       1.00      1.00      1.00       492\n",
            "     disgust       1.00      1.00      1.00       654\n",
            "        fear       1.00      1.00      1.00       317\n",
            "         joy       1.00      1.00      1.00       642\n",
            "     sadness       1.00      1.00      1.00       309\n",
            "    surprise       1.00      1.00      1.00       421\n",
            "\n",
            "   micro avg       1.00      1.00      1.00      2835\n",
            "   macro avg       1.00      1.00      1.00      2835\n",
            "weighted avg       1.00      1.00      1.00      2835\n",
            " samples avg       1.00      1.00      1.00      2835\n",
            "\n",
            "Macro F1 Score for Train Set: 1.0\n",
            "\n",
            "Evaluation on Test Set:\n",
            "\n",
            "Model: Logistic Regression - Test Set\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       anger       0.49      0.65      0.56        34\n",
            "     disgust       0.77      0.75      0.76        65\n",
            "        fear       0.97      0.88      0.92        32\n",
            "         joy       0.79      0.78      0.79        54\n",
            "     sadness       0.84      0.84      0.84        31\n",
            "    surprise       0.67      0.50      0.57        40\n",
            "\n",
            "   micro avg       0.74      0.73      0.74       256\n",
            "   macro avg       0.75      0.73      0.74       256\n",
            "weighted avg       0.75      0.73      0.74       256\n",
            " samples avg       0.72      0.77      0.70       256\n",
            "\n",
            "Macro F1 Score for Test Set: 0.7383116187503685\n",
            "\n",
            "Summary for Logistic Regression:\n",
            "  Train Macro F1 Score: 1.0\n",
            "  Test Macro F1 Score:  0.7383116187503685\n",
            "==============================================\n",
            "\n",
            "==============================================\n",
            "Training Random Forest...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation on Train Set:\n",
            "\n",
            "Model: Random Forest - Train Set\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       anger       1.00      0.98      0.99       492\n",
            "     disgust       1.00      1.00      1.00       654\n",
            "        fear       1.00      0.95      0.97       317\n",
            "         joy       1.00      1.00      1.00       642\n",
            "     sadness       1.00      0.89      0.94       309\n",
            "    surprise       1.00      0.85      0.92       421\n",
            "\n",
            "   micro avg       1.00      0.95      0.98      2835\n",
            "   macro avg       1.00      0.94      0.97      2835\n",
            "weighted avg       1.00      0.95      0.98      2835\n",
            " samples avg       0.98      0.96      0.97      2835\n",
            "\n",
            "Macro F1 Score for Train Set: 0.9701591683481902\n",
            "\n",
            "Evaluation on Test Set:\n",
            "\n",
            "Model: Random Forest - Test Set\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       anger       0.67      0.24      0.35        34\n",
            "     disgust       0.87      0.42      0.56        65\n",
            "        fear       1.00      0.28      0.44        32\n",
            "         joy       0.86      0.46      0.60        54\n",
            "     sadness       1.00      0.16      0.28        31\n",
            "    surprise       1.00      0.05      0.10        40\n",
            "\n",
            "   micro avg       0.86      0.30      0.44       256\n",
            "   macro avg       0.90      0.27      0.39       256\n",
            "weighted avg       0.89      0.30      0.42       256\n",
            " samples avg       0.37      0.32      0.34       256\n",
            "\n",
            "Macro F1 Score for Test Set: 0.3874626647950857\n",
            "\n",
            "Summary for Random Forest:\n",
            "  Train Macro F1 Score: 0.9701591683481902\n",
            "  Test Macro F1 Score:  0.3874626647950857\n",
            "==============================================\n",
            "\n",
            "Example prediction using SVM:\n",
            "anger: 0\n",
            "disgust: 0\n",
            "fear: 0\n",
            "joy: 1\n",
            "sadness: 0\n",
            "surprise: 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ukranian"
      ],
      "metadata": {
        "id": "5mjm6XSuXaRI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Import classifiers and wrappers\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.multioutput import MultiOutputClassifier\n",
        "\n",
        "# Evaluation metrics\n",
        "from sklearn.metrics import classification_report, f1_score\n",
        "\n",
        "# For generating text embeddings\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# For scaling the embeddings\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# -------------------------------\n",
        "# 1. Load Data and Generate Embeddings\n",
        "# -------------------------------\n",
        "\n",
        "# Load datasets\n",
        "train_df = pd.read_csv(\"/content/ukr.csv\")\n",
        "test_df = pd.read_csv(\"/content/ukr_dev.csv\")\n",
        "\n",
        "# Load the LaBSE model for generating embeddings\n",
        "labse_model = SentenceTransformer('intfloat/multilingual-e5-large')\n",
        "\n",
        "def generate_embeddings(texts):\n",
        "    \"\"\"\n",
        "    Given a list or array of texts, generate and return their normalized embeddings.\n",
        "    \"\"\"\n",
        "    return np.array(labse_model.encode(texts, convert_to_numpy=True, normalize_embeddings=True))\n",
        "\n",
        "# Get text data\n",
        "train_texts = train_df['text'].values\n",
        "test_texts = test_df['text'].values\n",
        "\n",
        "# Generate embeddings for train and test sets\n",
        "train_embeddings = generate_embeddings(train_texts)\n",
        "test_embeddings = generate_embeddings(test_texts)\n",
        "\n",
        "# -------------------------------\n",
        "# 2. Prepare Labels and Scale Features\n",
        "# -------------------------------\n",
        "\n",
        "# Define the emotion label columns\n",
        "label_columns = ['anger', 'disgust', 'fear', 'joy', 'sadness', 'surprise']\n",
        "\n",
        "# Extract label arrays\n",
        "train_labels = train_df[label_columns].values\n",
        "test_labels = test_df[label_columns].values\n",
        "\n",
        "# Scale the embeddings (feature standardization)\n",
        "scaler = StandardScaler()\n",
        "train_embeddings_scaled = scaler.fit_transform(train_embeddings)\n",
        "test_embeddings_scaled = scaler.transform(test_embeddings)\n",
        "\n",
        "# -------------------------------\n",
        "# 3. Define, Train, and Evaluate Models\n",
        "# -------------------------------\n",
        "\n",
        "# Define the four models. For Random Forest we add some regularization\n",
        "# by limiting the maximum tree depth and controlling the minimum samples needed for splits.\n",
        "models = {\n",
        "    \"SVM\": MultiOutputClassifier(SVC(kernel='rbf', probability=True, random_state=42)),\n",
        "    \"Naive Bayes\": MultiOutputClassifier(GaussianNB()),\n",
        "    \"Logistic Regression\": MultiOutputClassifier(\n",
        "        LogisticRegression(max_iter=1000, random_state=42)\n",
        "    ),\n",
        "    \"Random Forest\": MultiOutputClassifier(\n",
        "        RandomForestClassifier(max_depth=10, min_samples_split=5, random_state=42)\n",
        "    )\n",
        "}\n",
        "\n",
        "def evaluate_model(model, embeddings, labels, dataset_name, model_name):\n",
        "    \"\"\"\n",
        "    Predict labels using the given model and embeddings, then print the classification report\n",
        "    and macro F1 score.\n",
        "    \"\"\"\n",
        "    predictions = model.predict(embeddings)\n",
        "    print(f\"\\nModel: {model_name} - {dataset_name}\")\n",
        "    print(\"Classification Report:\")\n",
        "    print(classification_report(labels, predictions, target_names=label_columns))\n",
        "\n",
        "    macro_f1 = f1_score(labels, predictions, average='macro')\n",
        "    print(f\"Macro F1 Score for {dataset_name}: {macro_f1}\\n\")\n",
        "    return macro_f1\n",
        "\n",
        "# Train and evaluate each model on both the training and testing sets.\n",
        "for model_name, clf in models.items():\n",
        "    print(\"==============================================\")\n",
        "    print(f\"Training {model_name}...\")\n",
        "    clf.fit(train_embeddings_scaled, train_labels)\n",
        "\n",
        "    # Evaluate on the Training Set\n",
        "    print(\"Evaluation on Train Set:\")\n",
        "    train_macro_f1 = evaluate_model(clf, train_embeddings_scaled, train_labels, \"Train Set\", model_name)\n",
        "\n",
        "    # Evaluate on the Testing Set\n",
        "    print(\"Evaluation on Test Set:\")\n",
        "    test_macro_f1 = evaluate_model(clf, test_embeddings_scaled, test_labels, \"Test Set\", model_name)\n",
        "\n",
        "    print(f\"Summary for {model_name}:\")\n",
        "    print(f\"  Train Macro F1 Score: {train_macro_f1}\")\n",
        "    print(f\"  Test Macro F1 Score:  {test_macro_f1}\")\n",
        "    print(\"==============================================\\n\")\n",
        "\n",
        "# -------------------------------\n",
        "# 4. Predict Emotions for New Text\n",
        "# -------------------------------\n",
        "\n",
        "def predict_emotions(text, model, scaler):\n",
        "    \"\"\"\n",
        "    Given a new text string, generate its embedding, scale it, and predict emotion labels.\n",
        "    Returns a dictionary mapping each emotion to its predicted label (0 or 1).\n",
        "    \"\"\"\n",
        "    # Generate embedding for the new text\n",
        "    embedding = generate_embeddings([text])\n",
        "    embedding_scaled = scaler.transform(embedding)\n",
        "\n",
        "    # Get predictions\n",
        "    predictions = model.predict(embedding_scaled)\n",
        "\n",
        "    # Map each label to its corresponding emotion\n",
        "    emotion_preds = {emotion: int(pred) for emotion, pred in zip(label_columns, predictions[0])}\n",
        "    return emotion_preds\n",
        "\n",
        "# Example usage for prediction using the SVM model (you can choose any of the trained models)\n",
        "example_text = \"यह एक उदाहरण वाक्य है\"\n",
        "print(\"Example prediction using SVM:\")\n",
        "svm_model = models[\"SVM\"]\n",
        "example_prediction = predict_emotions(example_text, svm_model, scaler)\n",
        "for emotion, pred in example_prediction.items():\n",
        "    print(f\"{emotion}: {pred}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BnHjNPQBXfaH",
        "outputId": "0416d2d5-6b4c-4b8d-f98a-713b056981a2"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==============================================\n",
            "Training SVM...\n",
            "Evaluation on Train Set:\n",
            "\n",
            "Model: SVM - Train Set\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       anger       1.00      0.26      0.41        98\n",
            "     disgust       1.00      0.28      0.44        86\n",
            "        fear       1.00      0.92      0.96       172\n",
            "         joy       0.99      0.87      0.93       412\n",
            "     sadness       1.00      0.79      0.88       333\n",
            "    surprise       1.00      0.69      0.82       196\n",
            "\n",
            "   micro avg       1.00      0.74      0.85      1297\n",
            "   macro avg       1.00      0.63      0.74      1297\n",
            "weighted avg       1.00      0.74      0.83      1297\n",
            " samples avg       0.39      0.38      0.38      1297\n",
            "\n",
            "Macro F1 Score for Train Set: 0.7378061431426016\n",
            "\n",
            "Evaluation on Test Set:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model: SVM - Test Set\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       anger       0.00      0.00      0.00        12\n",
            "     disgust       0.00      0.00      0.00         9\n",
            "        fear       1.00      0.65      0.79        17\n",
            "         joy       0.67      0.44      0.53        41\n",
            "     sadness       0.79      0.32      0.46        34\n",
            "    surprise       0.75      0.16      0.26        19\n",
            "\n",
            "   micro avg       0.77      0.33      0.46       132\n",
            "   macro avg       0.53      0.26      0.34       132\n",
            "weighted avg       0.65      0.33      0.42       132\n",
            " samples avg       0.17      0.17      0.17       132\n",
            "\n",
            "Macro F1 Score for Test Set: 0.3390548248284821\n",
            "\n",
            "Summary for SVM:\n",
            "  Train Macro F1 Score: 0.7378061431426016\n",
            "  Test Macro F1 Score:  0.3390548248284821\n",
            "==============================================\n",
            "\n",
            "==============================================\n",
            "Training Naive Bayes...\n",
            "Evaluation on Train Set:\n",
            "\n",
            "Model: Naive Bayes - Train Set\n",
            "Classification Report:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "       anger       0.37      0.94      0.53        98\n",
            "     disgust       0.28      0.90      0.43        86\n",
            "        fear       0.59      0.91      0.72       172\n",
            "         joy       0.61      0.83      0.71       412\n",
            "     sadness       0.48      0.83      0.61       333\n",
            "    surprise       0.41      0.82      0.55       196\n",
            "\n",
            "   micro avg       0.48      0.85      0.61      1297\n",
            "   macro avg       0.46      0.87      0.59      1297\n",
            "weighted avg       0.51      0.85      0.63      1297\n",
            " samples avg       0.36      0.43      0.38      1297\n",
            "\n",
            "Macro F1 Score for Train Set: 0.5889125505626257\n",
            "\n",
            "Evaluation on Test Set:\n",
            "\n",
            "Model: Naive Bayes - Test Set\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       anger       0.40      0.50      0.44        12\n",
            "     disgust       0.13      0.33      0.19         9\n",
            "        fear       0.56      0.88      0.68        17\n",
            "         joy       0.44      0.68      0.53        41\n",
            "     sadness       0.39      0.71      0.50        34\n",
            "    surprise       0.36      0.63      0.46        19\n",
            "\n",
            "   micro avg       0.39      0.67      0.49       132\n",
            "   macro avg       0.38      0.62      0.47       132\n",
            "weighted avg       0.40      0.67      0.50       132\n",
            " samples avg       0.28      0.34      0.30       132\n",
            "\n",
            "Macro F1 Score for Test Set: 0.46810573685573686\n",
            "\n",
            "Summary for Naive Bayes:\n",
            "  Train Macro F1 Score: 0.5889125505626257\n",
            "  Test Macro F1 Score:  0.46810573685573686\n",
            "==============================================\n",
            "\n",
            "==============================================\n",
            "Training Logistic Regression...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation on Train Set:\n",
            "\n",
            "Model: Logistic Regression - Train Set\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       anger       1.00      1.00      1.00        98\n",
            "     disgust       1.00      1.00      1.00        86\n",
            "        fear       1.00      1.00      1.00       172\n",
            "         joy       1.00      1.00      1.00       412\n",
            "     sadness       1.00      1.00      1.00       333\n",
            "    surprise       1.00      1.00      1.00       196\n",
            "\n",
            "   micro avg       1.00      1.00      1.00      1297\n",
            "   macro avg       1.00      1.00      1.00      1297\n",
            "weighted avg       1.00      1.00      1.00      1297\n",
            " samples avg       0.50      0.50      0.50      1297\n",
            "\n",
            "Macro F1 Score for Train Set: 1.0\n",
            "\n",
            "Evaluation on Test Set:\n",
            "\n",
            "Model: Logistic Regression - Test Set\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       anger       0.31      0.33      0.32        12\n",
            "     disgust       0.20      0.11      0.14         9\n",
            "        fear       0.88      0.82      0.85        17\n",
            "         joy       0.45      0.46      0.46        41\n",
            "     sadness       0.50      0.53      0.51        34\n",
            "    surprise       0.37      0.37      0.37        19\n",
            "\n",
            "   micro avg       0.48      0.48      0.48       132\n",
            "   macro avg       0.45      0.44      0.44       132\n",
            "weighted avg       0.48      0.48      0.48       132\n",
            " samples avg       0.23      0.25      0.24       132\n",
            "\n",
            "Macro F1 Score for Test Set: 0.4419800139267482\n",
            "\n",
            "Summary for Logistic Regression:\n",
            "  Train Macro F1 Score: 1.0\n",
            "  Test Macro F1 Score:  0.4419800139267482\n",
            "==============================================\n",
            "\n",
            "==============================================\n",
            "Training Random Forest...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation on Train Set:\n",
            "\n",
            "Model: Random Forest - Train Set\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       anger       1.00      0.01      0.02        98\n",
            "     disgust       0.00      0.00      0.00        86\n",
            "        fear       1.00      0.74      0.85       172\n",
            "         joy       1.00      0.85      0.92       412\n",
            "     sadness       1.00      0.70      0.82       333\n",
            "    surprise       1.00      0.35      0.52       196\n",
            "\n",
            "   micro avg       1.00      0.60      0.75      1297\n",
            "   macro avg       0.83      0.44      0.52      1297\n",
            "weighted avg       0.93      0.60      0.69      1297\n",
            " samples avg       0.31      0.31      0.31      1297\n",
            "\n",
            "Macro F1 Score for Train Set: 0.520787661961584\n",
            "\n",
            "Evaluation on Test Set:\n",
            "\n",
            "Model: Random Forest - Test Set\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       anger       0.00      0.00      0.00        12\n",
            "     disgust       0.00      0.00      0.00         9\n",
            "        fear       0.00      0.00      0.00        17\n",
            "         joy       0.67      0.05      0.09        41\n",
            "     sadness       0.00      0.00      0.00        34\n",
            "    surprise       0.00      0.00      0.00        19\n",
            "\n",
            "   micro avg       0.67      0.02      0.03       132\n",
            "   macro avg       0.11      0.01      0.02       132\n",
            "weighted avg       0.21      0.02      0.03       132\n",
            " samples avg       0.01      0.01      0.01       132\n",
            "\n",
            "Macro F1 Score for Test Set: 0.015151515151515152\n",
            "\n",
            "Summary for Random Forest:\n",
            "  Train Macro F1 Score: 0.520787661961584\n",
            "  Test Macro F1 Score:  0.015151515151515152\n",
            "==============================================\n",
            "\n",
            "Example prediction using SVM:\n",
            "anger: 0\n",
            "disgust: 0\n",
            "fear: 0\n",
            "joy: 0\n",
            "sadness: 0\n",
            "surprise: 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Uvmk3_iWYLo4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# english"
      ],
      "metadata": {
        "id": "tHdvVpi1YUpv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Import classifiers and wrappers\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.multioutput import MultiOutputClassifier\n",
        "\n",
        "# Evaluation metrics\n",
        "from sklearn.metrics import classification_report, f1_score\n",
        "\n",
        "# For generating text embeddings\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# For scaling the embeddings\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# -------------------------------\n",
        "# 1. Load Data and Generate Embeddings\n",
        "# -------------------------------\n",
        "\n",
        "# Load datasets\n",
        "train_df = pd.read_csv(\"/content/esp.csv\")\n",
        "test_df = pd.read_csv(\"/content/esp_dev.csv\")\n",
        "\n",
        "# Load the LaBSE model for generating embeddings\n",
        "labse_model = SentenceTransformer('intfloat/multilingual-e5-large')\n",
        "\n",
        "def generate_embeddings(texts):\n",
        "    \"\"\"\n",
        "    Given a list or array of texts, generate and return their normalized embeddings.\n",
        "    \"\"\"\n",
        "    return np.array(labse_model.encode(texts, convert_to_numpy=True, normalize_embeddings=True))\n",
        "\n",
        "# Get text data\n",
        "train_texts = train_df['text'].values\n",
        "test_texts = test_df['text'].values\n",
        "\n",
        "# Generate embeddings for train and test sets\n",
        "train_embeddings = generate_embeddings(train_texts)\n",
        "test_embeddings = generate_embeddings(test_texts)\n",
        "\n",
        "# -------------------------------\n",
        "# 2. Prepare Labels and Scale Features\n",
        "# -------------------------------\n",
        "\n",
        "# Define the expected emotion labels.\n",
        "expected_labels = ['anger', 'disgust', 'fear', 'joy', 'sadness', 'surprise']\n",
        "\n",
        "# Create label_columns by including only those labels that exist in both datasets.\n",
        "label_columns = [label for label in expected_labels if label in train_df.columns and label in test_df.columns]\n",
        "\n",
        "# Optionally, print which labels will be used.\n",
        "print(\"Using emotion labels:\", label_columns)\n",
        "\n",
        "# Extract label arrays\n",
        "train_labels = train_df[label_columns].values\n",
        "test_labels = test_df[label_columns].values\n",
        "\n",
        "# Scale the embeddings (feature standardization)\n",
        "scaler = StandardScaler()\n",
        "train_embeddings_scaled = scaler.fit_transform(train_embeddings)\n",
        "test_embeddings_scaled = scaler.transform(test_embeddings)\n",
        "\n",
        "# -------------------------------\n",
        "# 3. Define, Train, and Evaluate Models\n",
        "# -------------------------------\n",
        "\n",
        "# Define the four models. For Random Forest we add some regularization\n",
        "# by limiting the maximum tree depth and controlling the minimum samples needed for splits.\n",
        "models = {\n",
        "    \"SVM\": MultiOutputClassifier(SVC(kernel='rbf', probability=True, random_state=42)),\n",
        "    \"Naive Bayes\": MultiOutputClassifier(GaussianNB()),\n",
        "    \"Logistic Regression\": MultiOutputClassifier(\n",
        "        LogisticRegression(max_iter=1000, random_state=42)\n",
        "    ),\n",
        "    \"Random Forest\": MultiOutputClassifier(\n",
        "        RandomForestClassifier(max_depth=10, min_samples_split=5, random_state=42)\n",
        "    )\n",
        "}\n",
        "\n",
        "def evaluate_model(model, embeddings, labels, dataset_name, model_name):\n",
        "    \"\"\"\n",
        "    Predict labels using the given model and embeddings, then print the classification report\n",
        "    and macro F1 score.\n",
        "    \"\"\"\n",
        "    predictions = model.predict(embeddings)\n",
        "    print(f\"\\nModel: {model_name} - {dataset_name}\")\n",
        "    print(\"Classification Report:\")\n",
        "    print(classification_report(labels, predictions, target_names=label_columns))\n",
        "\n",
        "    macro_f1 = f1_score(labels, predictions, average='macro')\n",
        "    print(f\"Macro F1 Score for {dataset_name}: {macro_f1}\\n\")\n",
        "    return macro_f1\n",
        "\n",
        "# Train and evaluate each model on both the training and testing sets.\n",
        "for model_name, clf in models.items():\n",
        "    print(\"==============================================\")\n",
        "    print(f\"Training {model_name}...\")\n",
        "    clf.fit(train_embeddings_scaled, train_labels)\n",
        "\n",
        "    # Evaluate on the Training Set\n",
        "    print(\"Evaluation on Train Set:\")\n",
        "    train_macro_f1 = evaluate_model(clf, train_embeddings_scaled, train_labels, \"Train Set\", model_name)\n",
        "\n",
        "    # Evaluate on the Testing Set\n",
        "    print(\"Evaluation on Test Set:\")\n",
        "    test_macro_f1 = evaluate_model(clf, test_embeddings_scaled, test_labels, \"Test Set\", model_name)\n",
        "\n",
        "    print(f\"Summary for {model_name}:\")\n",
        "    print(f\"  Train Macro F1 Score: {train_macro_f1}\")\n",
        "    print(f\"  Test Macro F1 Score:  {test_macro_f1}\")\n",
        "    print(\"==============================================\\n\")\n",
        "\n",
        "# -------------------------------\n",
        "# 4. Predict Emotions for New Text\n",
        "# -------------------------------\n",
        "\n",
        "def predict_emotions(text, model, scaler):\n",
        "    \"\"\"\n",
        "    Given a new text string, generate its embedding, scale it, and predict emotion labels.\n",
        "    Returns a dictionary mapping each available emotion to its predicted label (0 or 1).\n",
        "    \"\"\"\n",
        "    # Generate embedding for the new text\n",
        "    embedding = generate_embeddings([text])\n",
        "    embedding_scaled = scaler.transform(embedding)\n",
        "\n",
        "    # Get predictions\n",
        "    predictions = model.predict(embedding_scaled)\n",
        "\n",
        "    # Map each label to its corresponding emotion\n",
        "    emotion_preds = {emotion: int(pred) for emotion, pred in zip(label_columns, predictions[0])}\n",
        "    return emotion_preds\n",
        "\n",
        "# Example usage for prediction using the SVM model (you can choose any of the trained models)\n",
        "example_text = \"यह एक उदाहरण वाक्य है\"\n",
        "print(\"Example prediction using SVM:\")\n",
        "svm_model = models[\"SVM\"]\n",
        "example_prediction = predict_emotions(example_text, svm_model, scaler)\n",
        "for emotion, pred in example_prediction.items():\n",
        "    print(f\"{emotion}: {pred}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9SlvZIZxYWDO",
        "outputId": "fa0a9b39-41dd-4426-8aa9-8bfdc087be32"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using emotion labels: ['anger', 'disgust', 'fear', 'joy', 'sadness', 'surprise']\n",
            "==============================================\n",
            "Training SVM...\n",
            "Evaluation on Train Set:\n",
            "\n",
            "Model: SVM - Train Set\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       anger       0.98      0.93      0.96       492\n",
            "     disgust       0.98      0.97      0.97       654\n",
            "        fear       0.98      0.94      0.96       317\n",
            "         joy       0.99      0.97      0.98       642\n",
            "     sadness       1.00      0.91      0.95       309\n",
            "    surprise       1.00      0.82      0.90       421\n",
            "\n",
            "   micro avg       0.99      0.93      0.96      2835\n",
            "   macro avg       0.99      0.92      0.95      2835\n",
            "weighted avg       0.99      0.93      0.96      2835\n",
            " samples avg       0.97      0.95      0.95      2835\n",
            "\n",
            "Macro F1 Score for Train Set: 0.9541033216494447\n",
            "\n",
            "Evaluation on Test Set:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model: SVM - Test Set\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       anger       0.63      0.65      0.64        34\n",
            "     disgust       0.85      0.72      0.78        65\n",
            "        fear       1.00      0.75      0.86        32\n",
            "         joy       0.88      0.83      0.86        54\n",
            "     sadness       1.00      0.68      0.81        31\n",
            "    surprise       0.87      0.33      0.47        40\n",
            "\n",
            "   micro avg       0.86      0.67      0.75       256\n",
            "   macro avg       0.87      0.66      0.74       256\n",
            "weighted avg       0.87      0.67      0.74       256\n",
            " samples avg       0.75      0.72      0.71       256\n",
            "\n",
            "Macro F1 Score for Test Set: 0.7359532979098197\n",
            "\n",
            "Summary for SVM:\n",
            "  Train Macro F1 Score: 0.9541033216494447\n",
            "  Test Macro F1 Score:  0.7359532979098197\n",
            "==============================================\n",
            "\n",
            "==============================================\n",
            "Training Naive Bayes...\n",
            "Evaluation on Train Set:\n",
            "\n",
            "Model: Naive Bayes - Train Set\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       anger       0.69      0.86      0.76       492\n",
            "     disgust       0.79      0.87      0.83       654\n",
            "        fear       0.91      0.87      0.89       317\n",
            "         joy       0.80      0.84      0.82       642\n",
            "     sadness       0.81      0.83      0.82       309\n",
            "    surprise       0.60      0.78      0.68       421\n",
            "\n",
            "   micro avg       0.75      0.84      0.80      2835\n",
            "   macro avg       0.77      0.84      0.80      2835\n",
            "weighted avg       0.76      0.84      0.80      2835\n",
            " samples avg       0.77      0.87      0.79      2835\n",
            "\n",
            "Macro F1 Score for Train Set: 0.8000074292389535\n",
            "\n",
            "Evaluation on Test Set:\n",
            "\n",
            "Model: Naive Bayes - Test Set\n",
            "Classification Report:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "       anger       0.53      0.82      0.64        34\n",
            "     disgust       0.78      0.75      0.77        65\n",
            "        fear       0.90      0.81      0.85        32\n",
            "         joy       0.80      0.81      0.81        54\n",
            "     sadness       0.77      0.87      0.82        31\n",
            "    surprise       0.56      0.62      0.59        40\n",
            "\n",
            "   micro avg       0.71      0.78      0.74       256\n",
            "   macro avg       0.72      0.78      0.75       256\n",
            "weighted avg       0.73      0.78      0.75       256\n",
            " samples avg       0.72      0.82      0.73       256\n",
            "\n",
            "Macro F1 Score for Test Set: 0.7459197898589555\n",
            "\n",
            "Summary for Naive Bayes:\n",
            "  Train Macro F1 Score: 0.8000074292389535\n",
            "  Test Macro F1 Score:  0.7459197898589555\n",
            "==============================================\n",
            "\n",
            "==============================================\n",
            "Training Logistic Regression...\n",
            "Evaluation on Train Set:\n",
            "\n",
            "Model: Logistic Regression - Train Set\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       anger       1.00      1.00      1.00       492\n",
            "     disgust       1.00      1.00      1.00       654\n",
            "        fear       1.00      1.00      1.00       317\n",
            "         joy       1.00      1.00      1.00       642\n",
            "     sadness       1.00      1.00      1.00       309\n",
            "    surprise       1.00      1.00      1.00       421\n",
            "\n",
            "   micro avg       1.00      1.00      1.00      2835\n",
            "   macro avg       1.00      1.00      1.00      2835\n",
            "weighted avg       1.00      1.00      1.00      2835\n",
            " samples avg       1.00      1.00      1.00      2835\n",
            "\n",
            "Macro F1 Score for Train Set: 1.0\n",
            "\n",
            "Evaluation on Test Set:\n",
            "\n",
            "Model: Logistic Regression - Test Set\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       anger       0.49      0.65      0.56        34\n",
            "     disgust       0.77      0.75      0.76        65\n",
            "        fear       0.97      0.88      0.92        32\n",
            "         joy       0.79      0.78      0.79        54\n",
            "     sadness       0.84      0.84      0.84        31\n",
            "    surprise       0.67      0.50      0.57        40\n",
            "\n",
            "   micro avg       0.74      0.73      0.74       256\n",
            "   macro avg       0.75      0.73      0.74       256\n",
            "weighted avg       0.75      0.73      0.74       256\n",
            " samples avg       0.72      0.77      0.70       256\n",
            "\n",
            "Macro F1 Score for Test Set: 0.7383116187503685\n",
            "\n",
            "Summary for Logistic Regression:\n",
            "  Train Macro F1 Score: 1.0\n",
            "  Test Macro F1 Score:  0.7383116187503685\n",
            "==============================================\n",
            "\n",
            "==============================================\n",
            "Training Random Forest...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation on Train Set:\n",
            "\n",
            "Model: Random Forest - Train Set\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       anger       1.00      0.98      0.99       492\n",
            "     disgust       1.00      1.00      1.00       654\n",
            "        fear       1.00      0.95      0.97       317\n",
            "         joy       1.00      1.00      1.00       642\n",
            "     sadness       1.00      0.89      0.94       309\n",
            "    surprise       1.00      0.85      0.92       421\n",
            "\n",
            "   micro avg       1.00      0.95      0.98      2835\n",
            "   macro avg       1.00      0.94      0.97      2835\n",
            "weighted avg       1.00      0.95      0.98      2835\n",
            " samples avg       0.98      0.96      0.97      2835\n",
            "\n",
            "Macro F1 Score for Train Set: 0.9701591683481902\n",
            "\n",
            "Evaluation on Test Set:\n",
            "\n",
            "Model: Random Forest - Test Set\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       anger       0.67      0.24      0.35        34\n",
            "     disgust       0.87      0.42      0.56        65\n",
            "        fear       1.00      0.28      0.44        32\n",
            "         joy       0.86      0.46      0.60        54\n",
            "     sadness       1.00      0.16      0.28        31\n",
            "    surprise       1.00      0.05      0.10        40\n",
            "\n",
            "   micro avg       0.86      0.30      0.44       256\n",
            "   macro avg       0.90      0.27      0.39       256\n",
            "weighted avg       0.89      0.30      0.42       256\n",
            " samples avg       0.37      0.32      0.34       256\n",
            "\n",
            "Macro F1 Score for Test Set: 0.3874626647950857\n",
            "\n",
            "Summary for Random Forest:\n",
            "  Train Macro F1 Score: 0.9701591683481902\n",
            "  Test Macro F1 Score:  0.3874626647950857\n",
            "==============================================\n",
            "\n",
            "Example prediction using SVM:\n",
            "anger: 0\n",
            "disgust: 0\n",
            "fear: 0\n",
            "joy: 1\n",
            "sadness: 0\n",
            "surprise: 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    }
  ]
}